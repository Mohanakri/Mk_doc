# Lambda S3 Cross-Account & Cross-Region Access Manual

## Scenario Overview
- **Account A**: Source account with Lambda function
- **Account B**: Target account with S3 bucket
- **Cross-Region**: Lambda in us-east-1, S3 in eu-west-1

---

## Method 1: Cross-Account Access via Bucket Policy

### Step 1: Setup S3 Bucket in Account B (Target)

#### Console Steps:
1. **Login to Account B**
2. **Go to S3 Console** → Create bucket
3. **Bucket name**: `account-b-cross-bucket`
4. **Region**: `eu-west-1`
5. **Create bucket**

#### Step 2: Configure Bucket Policy in Account B

1. **Select bucket** → **Permissions** tab
2. **Bucket policy** → Edit
3. **Add policy**:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "CrossAccountLambdaAccess",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::ACCOUNT-A-ID:role/lambda-cross-account-role"
            },
            "Action": [
                "s3:ListBucket",
                "s3:GetObject",
                "s3:PutObject"
            ],
            "Resource": [
                "arn:aws:s3:::account-b-cross-bucket",
                "arn:aws:s3:::account-b-cross-bucket/*"
            ]
        }
    ]
}
```

### Step 3: Create IAM Role in Account A (Source)

#### Console Steps:
1. **Login to Account A**
2. **IAM Console** → **Roles** → **Create role**
3. **Trusted entity**: AWS service → Lambda
4. **Permissions**: Create custom policy

#### Custom Policy for Account A Role:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetObject",
                "s3:PutObject"
            ],
            "Resource": [
                "arn:aws:s3:::account-b-cross-bucket",
                "arn:aws:s3:::account-b-cross-bucket/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "*"
        }
    ]
}
```

5. **Role name**: `lambda-cross-account-role`

### Step 4: Create Lambda Function in Account A

#### Console Steps:
1. **Lambda Console** → **Create function**
2. **Function name**: `cross-account-s3-access`
3. **Runtime**: Python 3.9
4. **Execution role**: Use existing role → `lambda-cross-account-role`
5. **Create function**

#### Python Code (Method 1):
```python
import json
import boto3
from botocore.exceptions import ClientError

def lambda_handler(event, context):
    # Cross-region S3 client
    s3_client = boto3.client('s3', region_name='eu-west-1')
    bucket_name = 'account-b-cross-bucket'
    
    try:
        # List objects in cross-account bucket
        response = s3_client.list_objects_v2(Bucket=bucket_name)
        
        objects = []
        if 'Contents' in response:
            objects = [obj['Key'] for obj in response['Contents']]
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Cross-account access successful',
                'bucket': bucket_name,
                'region': 'eu-west-1',
                'object_count': len(objects),
                'objects': objects[:10]  # First 10 objects
            })
        }
        
    except ClientError as e:
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': f'Cross-account access failed: {str(e)}'
            })
        }
```

---

## Method 2: Cross-Account Access via AssumeRole

### Step 1: Create Cross-Account Role in Account B

#### Console Steps (Account B):
1. **IAM Console** → **Roles** → **Create role**
2. **Trusted entity**: Another AWS account
3. **Account ID**: Enter Account A's ID
4. **Permissions**: Create custom policy

#### S3 Access Policy for Cross-Account Role:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::account-b-cross-bucket",
                "arn:aws:s3:::account-b-cross-bucket/*"
            ]
        }
    ]
}
```

5. **Role name**: `cross-account-s3-role`
6. **Copy Role ARN**: `arn:aws:iam::ACCOUNT-B-ID:role/cross-account-s3-role`

### Step 2: Update Lambda Role in Account A

#### Console Steps (Account A):
1. **IAM Console** → **Roles** → Select lambda role
2. **Add permissions** → Create inline policy

#### AssumeRole Policy:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "sts:AssumeRole",
            "Resource": "arn:aws:iam::ACCOUNT-B-ID:role/cross-account-s3-role"
        }
    ]
}
```

### Step 3: Lambda Code with AssumeRole

#### Python Code (Method 2):
```python
import json
import boto3
from botocore.exceptions import ClientError

def lambda_handler(event, context):
    # STS client to assume role
    sts_client = boto3.client('sts')
    
    try:
        # Assume role in Account B
        assumed_role = sts_client.assume_role(
            RoleArn='arn:aws:iam::ACCOUNT-B-ID:role/cross-account-s3-role',
            RoleSessionName='lambda-cross-account-session'
        )
        
        # Get temporary credentials
        credentials = assumed_role['Credentials']
        
        # S3 client with assumed role credentials
        s3_client = boto3.client(
            's3',
            region_name='eu-west-1',
            aws_access_key_id=credentials['AccessKeyId'],
            aws_secret_access_key=credentials['SecretAccessKey'],
            aws_session_token=credentials['SessionToken']
        )
        
        bucket_name = 'account-b-cross-bucket'
        
        # List objects using assumed role
        response = s3_client.list_objects_v2(Bucket=bucket_name)
        
        objects = []
        if 'Contents' in response:
            objects = [obj['Key'] for obj in response['Contents']]
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'AssumeRole cross-account access successful',
                'method': 'AssumeRole',
                'bucket': bucket_name,
                'region': 'eu-west-1',
                'object_count': len(objects),
                'objects': objects[:10],
                'session': assumed_role['AssumedRoleUser']['AssumedRoleId']
            })
        }
        
    except ClientError as e:
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': f'AssumeRole failed: {str(e)}'
            })
        }
```

---

## S3 Cross-Region Replication Setup

### Step 1: Enable Versioning on Source Bucket

#### Console Steps (Account A):
1. **S3 Console** → Select source bucket
2. **Properties** → **Bucket Versioning** → **Edit**
3. **Enable** versioning

### Step 2: Create Destination Bucket

#### Console Steps (Account B):
1. **Create bucket**: `replica-destination-bucket`
2. **Region**: Different from source (e.g., `ap-south-1`)
3. **Enable versioning**

### Step 3: Create Replication Rule

#### Console Steps (Account A - Source Bucket):
1. **Management** → **Replication rules** → **Create replication rule**
2. **Rule name**: `cross-account-replication`
3. **Status**: Enabled
4. **Source**: Entire bucket
5. **Destination**: 
   - **Bucket**: `arn:aws:s3:::replica-destination-bucket`
   - **Account**: Account B ID
6. **IAM role**: Create new role or use existing

#### Replication IAM Policy:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObjectVersionForReplication",
                "s3:GetObjectVersionAcl"
            ],
            "Resource": "arn:aws:s3:::source-bucket/*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ReplicateObject",
                "s3:ReplicateDelete"
            ],
            "Resource": "arn:aws:s3:::replica-destination-bucket/*"
        }
    ]
}
```

---

## Lambda Code to Test Replication

```python
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):
    # Source bucket client
    s3_source = boto3.client('s3', region_name='us-east-1')
    # Destination bucket client (cross-account)
    s3_dest = boto3.client('s3', region_name='ap-south-1')
    
    source_bucket = 'source-bucket'
    dest_bucket = 'replica-destination-bucket'
    
    try:
        # Upload test file to source
        test_key = f'test-replication-{datetime.now().strftime("%Y%m%d-%H%M%S")}.txt'
        s3_source.put_object(
            Bucket=source_bucket,
            Key=test_key,
            Body='Test replication content'
        )
        
        # List objects in both buckets
        source_objects = s3_source.list_objects_v2(Bucket=source_bucket)
        source_count = len(source_objects.get('Contents', []))
        
        # For destination, use assumed role or bucket policy access
        dest_objects = s3_dest.list_objects_v2(Bucket=dest_bucket)
        dest_count = len(dest_objects.get('Contents', []))
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Replication test completed',
                'source_bucket': source_bucket,
                'destination_bucket': dest_bucket,
                'source_objects': source_count,
                'dest_objects': dest_count,
                'test_file': test_key
            })
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }
```

---

## Testing Steps

### Test Cross-Account Access:
1. **Deploy Lambda** in Account A
2. **Create test event** in Lambda console
3. **Execute function**
4. **Check CloudWatch logs** for results
5. **Verify S3 access** in Account B

### Test Replication:
1. **Upload file** to source bucket
2. **Wait 15 minutes** for replication
3. **Check destination bucket** in Account B
4. **Verify object exists** with same content

---

## Common Issues & Troubleshooting

### Access Denied Errors:
- **Check bucket policy** in destination account
- **Verify IAM role permissions** in source account
- **Confirm account IDs** are correct

### Replication Not Working:
- **Enable versioning** on both buckets
- **Check replication IAM role** permissions
- **Verify different regions** for source and destination

### Cross-Region Latency:
- **Use regional endpoints** for better performance
- **Consider CloudFront** for frequently accessed objects

---

## Security Best Practices

1. **Use least privilege** principle for all IAM policies
2. **Enable CloudTrail** for audit logging
3. **Use VPC endpoints** for private connectivity
4. **Encrypt data** in transit and at rest
5. **Regular review** of cross-account permissions

----------------

# S3 to EFS Mount & Comprehensive S3 Interview Questions

## S3 Mount to EFS - Methods & Setup

### Method 1: S3FS-FUSE (Direct S3 Mount)

#### Installation Steps:
```bash
# Install s3fs-fuse on EC2/EFS Mount Target
sudo yum update -y
sudo yum install -y s3fs-fuse

# Create mount point
sudo mkdir /mnt/s3-bucket

# Mount S3 bucket
s3fs your-bucket-name /mnt/s3-bucket -o iam_role=auto,allow_other
```

#### Python Code to Test Mount:
```python
import os
import boto3

def test_s3_mount():
    mount_path = '/mnt/s3-bucket'
    
    # Write file through mount
    with open(f'{mount_path}/test-file.txt', 'w') as f:
        f.write('Hello from S3 mount!')
    
    # Read file through mount
    with open(f'{mount_path}/test-file.txt', 'r') as f:
        content = f.read()
    
    # List files in mount
    files = os.listdir(mount_path)
    
    return {
        'content': content,
        'files': files
    }
```

### Method 2: S3 Sync to EFS via Lambda

#### Lambda Function for S3 to EFS Sync:
```python
import json
import boto3
import os
import subprocess

def lambda_handler(event, context):
    s3_bucket = event['bucket']
    efs_mount = '/mnt/efs'
    
    # AWS CLI sync command
    sync_command = f'aws s3 sync s3://{s3_bucket}/ {efs_mount}/'
    
    try:
        result = subprocess.run(sync_command, shell=True, capture_output=True, text=True)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Sync completed',
                'output': result.stdout,
                'errors': result.stderr
            })
        }
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
```

### Method 3: DataSync for S3 to EFS

#### Console Steps:
1. **DataSync Console** → **Create task**
2. **Source**: S3 bucket location
3. **Destination**: EFS file system
4. **Configure options**: Preserve metadata, verify data
5. **Schedule**: One-time or recurring

---
