# Complete S3 Hands-On Learning Path - All Features

## **Prerequisites Setup**
- AWS Account with appropriate permissions
- AWS CLI installed and configured
- Basic understanding of AWS Console

---

## **Phase 1: S3 Fundamentals (Beginner)**

### **Task 1.1: Basic Bucket Operations**
**Objective**: Master basic bucket creation and management

**Steps**:
1. **Create buckets via Console**:
   - Create bucket: `your-name-demo-bucket-2024`
   - Try different regions (us-east-1, eu-west-1)
   - Test bucket naming rules (invalid names, duplicate names)

2. **Create buckets via CLI**:
   ```bash
   aws s3 mb s3://your-name-cli-bucket-2024 --region us-west-2
   aws s3 ls
   ```

3. **Bucket Properties Exploration**:
   - View bucket properties in console
   - Note bucket ARN, region, creation date

**Expected Outcome**: Understanding of bucket creation, naming conventions, and regional placement

---

### **Task 1.2: Object Upload and Management**
**Objective**: Master object operations

**Steps**:
1. **Upload via Console**:
   - Create test files: `sample.txt`, `image.jpg`, `document.pdf`
   - Upload to different folders: `/documents/`, `/images/`, `/backup/`
   - Try uploading 0-byte file and large file (>100MB)

2. **Upload via CLI**:
   ```bash
   # Single file
   aws s3 cp sample.txt s3://your-bucket/test/
   
   # Multiple files
   aws s3 sync ./local-folder s3://your-bucket/sync-test/
   
   # With metadata
   aws s3 cp file.txt s3://your-bucket/ --metadata project=demo,owner=yourname
   ```

3. **Object Management**:
   - Rename objects (copy + delete)
   - Move objects between folders
   - Download objects to local machine

**Expected Outcome**: Proficiency in object upload, organization, and retrieval

---

### **Task 1.3: Storage Classes Hands-On**
**Objective**: Understand and implement different storage classes

**Steps**:
1. **Upload objects with different storage classes**:
   - Standard: Frequently accessed files
   - Standard-IA: Monthly reports
   - One Zone-IA: Reproducible data
   - Glacier Instant Retrieval: Quarterly archives
   - Glacier Flexible Retrieval: Annual backups
   - Glacier Deep Archive: Compliance data

2. **CLI Storage Class Upload**:
   ```bash
   aws s3 cp file.txt s3://bucket/ --storage-class STANDARD_IA
   aws s3 cp archive.zip s3://bucket/ --storage-class GLACIER
   ```

3. **Cost Analysis**:
   - Use AWS Pricing Calculator
   - Compare costs for 1TB data across classes
   - Document findings

**Expected Outcome**: Understanding of storage class economics and use cases

---

## **Phase 2: Access Control and Security (Intermediate)**

### **Task 2.1: Bucket Policies Deep Dive**
**Objective**: Master bucket-level access control

**Steps**:
1. **Create Different Policy Scenarios**:
   
   **Public Read Policy**:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Sid": "PublicReadGetObject",
         "Effect": "Allow",
         "Principal": "*",
         "Action": "s3:GetObject",
         "Resource": "arn:aws:s3:::your-bucket/*"
       }
     ]
   }
   ```

   **IP Restriction Policy**:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Deny",
         "Principal": "*",
         "Action": "s3:*",
         "Resource": [
           "arn:aws:s3:::your-bucket",
           "arn:aws:s3:::your-bucket/*"
         ],
         "Condition": {
           "NotIpAddress": {
             "aws:SourceIp": "YOUR-IP-ADDRESS/32"
           }
         }
       }
     ]
   }
   ```

2. **Test Access**:
   - Test from allowed/denied IPs
   - Use different browsers/incognito mode
   - Test with AWS CLI from different locations

**Expected Outcome**: Mastery of bucket policies and conditional access

---

### **Task 2.2: IAM Integration**
**Objective**: Integrate S3 with IAM users and roles

**Steps**:
1. **Create IAM Users**:
   - `s3-readonly-user`: Read-only access to specific bucket
   - `s3-upload-user`: Upload access to specific folder
   - `s3-admin-user`: Full access to specific bucket

2. **Create IAM Policies**:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": [
           "s3:GetObject",
           "s3:PutObject"
         ],
         "Resource": "arn:aws:s3:::your-bucket/uploads/*"
       }
     ]
   }
   ```

3. **Test with Different Users**:
   - Configure AWS CLI profiles for each user
   - Test access boundaries
   - Document permission patterns

**Expected Outcome**: Understanding IAM-S3 integration patterns

---

### **Task 2.3: Advanced Access Controls**
**Objective**: Implement ACLs and advanced security features

**Steps**:
1. **Object ACLs**:
   - Grant specific user read access to object
   - Grant cross-account access via ACL
   - Test canonical user ID vs email address

2. **Block Public Access**:
   - Enable at bucket level
   - Test override attempts
   - Enable at account level

3. **Access Logging**:
   - Enable server access logging
   - Create log analysis bucket
   - Analyze log patterns

**Expected Outcome**: Comprehensive access control implementation

---

## **Phase 3: Advanced Features (Advanced)**

### **Task 3.1: Versioning and Lifecycle Management**
**Objective**: Master version control and automated management

**Steps**:
1. **Enable Versioning**:
   - Upload same file multiple times
   - View version history
   - Restore previous version
   - Delete specific version vs delete marker

2. **Lifecycle Policies**:
   ```json
   {
     "Rules": [
       {
         "ID": "TransitionRule",
         "Status": "Enabled",
         "Filter": {"Prefix": "documents/"},
         "Transitions": [
           {
             "Days": 30,
             "StorageClass": "STANDARD_IA"
           },
           {
             "Days": 90,
             "StorageClass": "GLACIER"
           },
           {
             "Days": 365,
             "StorageClass": "DEEP_ARCHIVE"
           }
         ],
         "Expiration": {
           "Days": 2555
         }
       }
     ]
   }
   ```

3. **Test Lifecycle**:
   - Create test objects with backdated timestamps
   - Monitor transitions (use CloudWatch Events)
   - Measure cost impact

**Expected Outcome**: Automated data lifecycle management

---

### **Task 3.2: Cross-Region Replication (CRR)**
**Objective**: Implement disaster recovery and compliance replication

**Steps**:
1. **Set Up CRR**:
   - Create source bucket in us-east-1
   - Create destination bucket in eu-west-1
   - Enable versioning on both
   - Create replication role

2. **Configure Replication Rule**:
   - Replicate entire bucket
   - Replicate specific prefix only
   - Change storage class during replication
   - Enable replica metadata sync

3. **Test Scenarios**:
   - Upload new objects
   - Modify existing objects
   - Delete objects (test delete marker replication)
   - Test cross-account replication

4. **Monitor Replication**:
   - Use CloudWatch metrics
   - Set up replication failure alerts
   - Test replication time control (RTC)

**Expected Outcome**: Robust disaster recovery setup

---

### **Task 3.3: Encryption Implementation**
**Objective**: Secure data at rest and in transit

**Steps**:
1. **Server-Side Encryption**:
   
   **SSE-S3**:
   ```bash
   aws s3 cp file.txt s3://bucket/ --server-side-encryption AES256
   ```

   **SSE-KMS**:
   ```bash
   aws s3 cp file.txt s3://bucket/ --server-side-encryption aws:kms --sse-kms-key-id your-key-id
   ```

   **SSE-C**:
   ```bash
   aws s3 cp file.txt s3://bucket/ --server-side-encryption-customer-algorithm AES256 --server-side-encryption-customer-key base64key --server-side-encryption-customer-key-md5 md5hash
   ```

2. **Client-Side Encryption**:
   - Use AWS SDK with encryption client
   - Implement custom encryption before upload
   - Test key management scenarios

3. **Bucket Default Encryption**:
   - Set default SSE-S3
   - Set default SSE-KMS
   - Test override scenarios

**Expected Outcome**: Comprehensive encryption strategy

---

### **Task 3.4: Performance Optimization**
**Objective**: Optimize S3 performance for high-throughput scenarios

**Steps**:
1. **Multipart Upload**:
   - Upload large files (>100MB) using multipart
   - Implement parallel uploads
   - Test failure and resume scenarios

2. **Transfer Acceleration**:
   - Enable on bucket
   - Test speed difference
   - Use accelerated endpoints

3. **Request Pattern Optimization**:
   - Implement request distribution across prefixes
   - Test hotspotting scenarios
   - Use CloudFront for read-heavy workloads

4. **Performance Testing**:
   ```bash
   # Parallel uploads
   aws s3 cp largefile.zip s3://bucket/ --storage-class STANDARD --cli-write-timeout 0
   
   # Sync with parallel processing
   aws s3 sync ./large-folder s3://bucket/ --delete --storage-class STANDARD
   ```

**Expected Outcome**: High-performance S3 operations

---

## **Phase 4: Modern S3 Features (Expert)**

### **Task 4.1: S3 Access Points**
**Objective**: Implement granular access control with access points

**Steps**:
1. **Create Access Points**:
   - Create access point for finance team
   - Create access point for engineering team
   - Create internet vs VPC access points

2. **Configure Policies**:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {"AWS": "arn:aws:iam::ACCOUNT:user/finance-user"},
         "Action": ["s3:GetObject", "s3:PutObject"],
         "Resource": "arn:aws:s3:region:account:accesspoint/finance-ap/object/finance/*"
       }
     ]
   }
   ```

3. **Test Access Patterns**:
   - Access via access point ARN
   - Test cross-account access
   - Monitor access point metrics

**Expected Outcome**: Sophisticated access management

---

### **Task 4.2: Multi-Region Access Points**
**Objective**: Create global access patterns

**Steps**:
1. **Set Up MRAP**:
   - Create buckets in multiple regions
   - Create multi-region access point
   - Configure failover routing

2. **Test Global Access**:
   - Access from different geographic locations
   - Test failover scenarios
   - Monitor request routing

3. **Performance Analysis**:
   - Measure latency from different regions
   - Test during regional failures
   - Document routing behavior

**Expected Outcome**: Global S3 architecture

---

### **Task 4.3: S3 Batch Operations**
**Objective**: Perform bulk operations at scale

**Steps**:
1. **Create Inventory**:
   - Generate S3 inventory report
   - Filter objects for batch processing
   - Export to CSV format

2. **Batch Jobs**:
   
   **Copy Job**:
   - Copy objects between buckets
   - Change storage class during copy
   - Add metadata during copy

   **Tag Job**:
   - Add tags to thousands of objects
   - Replace existing tags
   - Delete specific tags

   **Lambda Job**:
   - Create Lambda function for custom processing
   - Process objects with custom logic
   - Handle errors and retries

3. **Monitor and Report**:
   - Track job progress
   - Analyze completion reports
   - Handle failed operations

**Expected Outcome**: Scalable bulk operations

---

### **Task 4.4: Object Lambda**
**Objective**: Transform data on retrieval

**Steps**:
1. **Create Lambda Function**:
   ```python
   import boto3
   import json
   
   def lambda_handler(event, context):
       # Transform object data
       s3 = boto3.client('s3')
       
       # Get original object
       response = s3.get_object(
           Bucket=event['inputS3Url']['bucket'],
           Key=event['inputS3Url']['key']
       )
       
       # Transform data (e.g., redact PII)
       original_data = response['Body'].read()
       transformed_data = transform_data(original_data)
       
       # Return transformed data
       return {
           'statusCode': 200,
           'body': transformed_data
       }
   ```

2. **Create Object Lambda Access Point**:
   - Link to supporting access point
   - Configure Lambda function
   - Set up IAM permissions

3. **Test Transformations**:
   - Retrieve objects via Object Lambda AP
   - Test different transformation scenarios
   - Monitor performance impact

**Expected Outcome**: Dynamic data transformation

---

## **Phase 5: Monitoring and Analytics (Expert)**

### **Task 5.1: Comprehensive Monitoring**
**Objective**: Set up complete observability

**Steps**:
1. **CloudWatch Metrics**:
   - Enable request metrics
   - Set up storage metrics
   - Create custom dashboards

2. **CloudTrail Integration**:
   - Enable data events logging
   - Analyze access patterns
   - Set up security alerts

3. **Cost Monitoring**:
   - Use Cost Explorer for S3 costs
   - Set up billing alerts
   - Analyze cost by storage class

**Expected Outcome**: Full operational visibility

---

### **Task 5.2: S3 Storage Lens**
**Objective**: Organization-wide storage analytics

**Steps**:
1. **Configure Storage Lens**:
   - Set up organization-wide configuration
   - Create custom metrics
   - Enable cost optimization insights

2. **Advanced Analytics**:
   - Analyze usage trends
   - Identify optimization opportunities
   - Generate executive reports

3. **Automation**:
   - Create automated optimization workflows
   - Set up alerts for anomalies
   - Implement cost governance policies

**Expected Outcome**: Strategic storage management

---

## **Phase 6: Integration Projects (Real-World Applications)**

### **Project 6.1: Build a Data Lake**
**Objective**: Create production-ready data lake

**Components**:
- Raw data ingestion bucket
- Processed data bucket with partitioning
- Athena integration for querying
- Glue catalog for metadata
- Lambda for data processing
- Step Functions for orchestration

### **Project 6.2: Content Distribution System**
**Objective**: Global content delivery

**Components**:
- S3 for origin storage
- CloudFront distribution
- Lambda@Edge for customization
- Route 53 for DNS
- WAF for security

### **Project 6.3: Backup and Archive System**
**Objective**: Enterprise backup solution

**Components**:
- Automated backup workflows
- Cross-region replication
- Glacier archive policies
- Restore automation
- Compliance reporting

### **Project 6.4: Static Website with CI/CD**
**Objective**: Modern web hosting

**Components**:
- S3 static hosting
- CloudFront distribution
- Route 53 custom domain
- CodePipeline for CI/CD
- Lambda for backend APIs

---

## **Assessment and Certification Prep**

### **Hands-On Scenarios for Practice**:
1. **Disaster Recovery Drill**: Simulate region failure and test recovery
2. **Security Audit**: Review and remediate security findings
3. **Cost Optimization**: Identify and implement cost savings
4. **Performance Tuning**: Optimize for high-throughput workloads
5. **Compliance Implementation**: Implement regulatory requirements

### **Final Capstone Project**:
Design and implement a complete S3-based solution that includes:
- Multi-tier storage strategy
- Cross-region replication
- Advanced security controls
- Monitoring and alerting
- Cost optimization
- Integration with other AWS services

---

## **Additional Resources**

### **CLI Commands Reference**:
```bash
# Bucket operations
aws s3 mb s3://bucket-name
aws s3 rb s3://bucket-name --force

# Object operations
aws s3 cp file.txt s3://bucket/
aws s3 sync ./folder s3://bucket/folder/
aws s3 rm s3://bucket/file.txt

# Advanced operations
aws s3api put-bucket-versioning --bucket bucket-name --versioning-configuration Status=Enabled
aws s3api put-bucket-replication --bucket source-bucket --replication-configuration file://replication.json
```

### **Useful Scripts and Tools**:
- S3 cost analysis scripts
- Bulk operations automation
- Security audit tools
- Performance testing utilities

This comprehensive hands-on guide will give you practical experience with every major S3 feature through progressive, real-world scenarios.