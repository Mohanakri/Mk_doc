# Python Benchmarking - Chapter 21 Notes

## Overview
This chapter explores Python's code-timing tools and benchmarking techniques by comparing the performance of iteration tools like list comprehensions, map calls, for loops, and generators.

## Key Benchmarking Principles

### Important Caveats
- **Code structure and host architecture** can influence speed arbitrarily
- **Python's performance varies over time** due to constant internal changes
- **Alternative Pythons** (like PyPy) have very different performance profiles
- **Benchmarking is empirical** - you must test on your own device with your Python version

### General Rule
> If you want to verify speed, you need to time your code, on your own device, with the Python(s) you plan to use, and in the here and now.

## Custom Timer Modules

### Timer Module: Take 1 (timer0.py)
```python
import time

def timer(func, *args):                    # Basic timer with limitations
    start = time.perf_counter()
    for i in range(100_000):               # Hardcoded reps
        func(*args)
    return time.perf_counter() - start
```

**Limitations:**
- Hardcodes repetitions at 100k
- Charges range() cost to tested function
- No keyword argument support
- No way to verify function worked
- Only gives total time

### Timer Module: Take 2 (timer.py)
```python
import time
timer = time.perf_counter

def once(func, *pargs, **kargs):
    """Time to run func(...) one time. Returns (time, result)."""
    start = timer()
    result = func(*pargs, **kargs)
    elapsed = timer() - start
    return (elapsed, result)

def total(reps, func, *pargs, **kargs):
    """Total time to run func(...) reps times."""
    total = 0
    for i in range(reps):
        time, result = once(func, *pargs, **kargs)
        total += time
    return (total, result)

def bestof(reps, func, *pargs, **kargs):
    """Best time among reps runs of func(...)."""
    return min(once(func, *pargs, **kargs) for i in range(reps))

def bestoftotal(reps1, reps2, func, *pargs, **kargs):
    """Best total time among reps1 runs of [reps2 runs of func(...)]."""
    return min(total(reps2, func, *pargs, **kargs) for i in range(reps1))
```

**Improvements:**
- Configurable repetitions
- Separates timing from range() construction
- Supports both positional and keyword arguments
- Returns function results for verification
- Multiple timing modes (once, total, bestof, bestoftotal)

### Usage Examples
```python
import timer

# Single call timing
timer.once(pow, 2, 1000)[0]  # Returns (time, result)

# Total time for multiple calls
timer.total(100_000, str.upper, 'hack' * 100)[0]

# Best time among multiple runs
timer.bestof(50, pow, 2, 1000)[0]

# Best of totals (recommended for accuracy)
timer.bestoftotal(50, 100_000, pow, 2, 1000)[0]
```

## Timing Runner System

### Timer Runner (timer_runner.py)
```python
import timer, sys

def runner(*tests):
    """Run multiple test functions and compare results."""
    results = []
    print('Python', sys.version.split()[0], 'on', sys.platform)
    
    # Time each test
    for test in tests:
        besttime, result = timer.bestoftotal(10, 1000, test)
        results.append(result)
        print(f'{test.__name__:<9}: '
              f'{besttime:.5f} => [{result[0]}…{result[-1]}]')
    
    # Verify all results are the same
    print('Results differ!'
           if any(result != results[0] for result in results[1:])
           else 'All results same.')
```

### Test Script Example (timer_tests.py)
```python
from timer_runner import runner

repslist = list(range(10_000))

def forLoop():
    res = []
    for x in repslist:
        res.append(abs(x))
    return res

def listComp():
    return [abs(x) for x in repslist]

def mapCall():
    return list(map(abs, repslist))

def genExpr():
    return list(abs(x) for x in repslist)

def genFunc():
    def gen():
        for x in repslist:
            yield abs(x)
    return list(gen())

runner(forLoop, listComp, mapCall, genExpr, genFunc)
```

## Performance Results

### CPython 3.12 Results (seconds)
```
forLoop  : 0.26035 => [0...9999]
listComp : 0.20781 => [0...9999]
mapCall  : 0.14399 => [0...9999]
genExpr  : 0.41133 => [0...9999]
genFunc  : 0.41203 => [0...9999]
```

**Rankings:** map > list comprehensions > for loops > generators

### PyPy 7.3 Results (seconds)
```
forLoop  : 0.04329 => [0...9999]
listComp : 0.01876 => [0...9999]
mapCall  : 0.04132 => [0...9999]
genExpr  : 0.08744 => [0...9999]
genFunc  : 0.08726 => [0...9999]
```

**Rankings:** list comprehensions > for loops ≈ map > generators
**Speed:** PyPy ~5X faster than CPython overall

### Key Findings
1. **List comprehensions** vs **Generator expressions**: `[abs(x) for x in items]` is much faster than `list(abs(x) for x in items)`
2. **Built-in functions**: map performs best when calling built-in functions like `abs`
3. **User-defined functions**: map performance drops significantly with lambdas or user functions
4. **Generators**: Consistently slowest due to state-saving overhead

### Impact of Function Types

#### With User-Defined Functions (+ operation)
```python
# CPython 3.12 results
forLoop  : 0.28682
listComp : 0.24389  # Winner
mapCall  : 0.49622  # Much slower due to lambda
genExpr  : 0.44047
genFunc  : 0.44476
```

#### With Simple User-Defined Function
```python
def F(x): return x

# CPython 3.12 results
forLoop  : 0.36206
listComp : 0.31181  # Winner
mapCall  : 0.35479  # Better than with lambda
genExpr  : 0.50531
genFunc  : 0.50290
```

## Enhanced Timer Module (timer2.py)

### Improved Interface
```python
def total(func, /, *pargs, _reps=100_000, **kargs):
    """Uses keyword-only arguments with defaults for reps."""
    total = 0
    for i in range(_reps):
        time, result = once(func, *pargs, **kargs)
        total += time
    return (total, result)

def bestoftotal(func, /, *pargs, _reps1=50, **kargs):
    """Positional-only for func, keyword-only for repetitions."""
    return min(total(func, *pargs, **kargs) for i in range(_reps1))
```

### Usage with Defaults
```python
# Use defaults
timer2.total(str.upper, 'hack' * 100)[0]

# Override defaults
timer2.bestoftotal(str.upper, 'hack' * 100, _reps=10_000)[0]

# Test functions with complex arguments
timer2.bestoftotal(f, 1, 2, c=66, d=77, _reps1=50, _reps=100_000)
```

## Python's timeit Module

### Basic Usage

#### API Mode
```python
import timeit

# Time a statement string
min(timeit.repeat(stmt='[x ** 2 for x in range(1000)]', 
                  number=1000, repeat=5))

# Result: Best total time among 5 runs of 1000 executions each
```

#### Command-Line Mode
```bash
# Basic timing
python3 -m timeit -n 1000 "[x ** 2 for x in range(1000)]"
# Output: 1000 loops, best of 5: 51.9 usec per loop

# With custom repeat count
python3 -m timeit -n 1000 -r 50 "[x ** 2 for x in range(1000)]"

# Using different timer
python3 -m timeit -p -n 1000 -r 50 "[x ** 2 for x in range(1000)]"
```

### Advanced timeit Features

#### Setup Code
```python
# API mode with setup
min(timeit.repeat(
    setup='from mins import min4\nvals=list(range(1000))',
    stmt='min4(*vals)',
    number=1000, repeat=5
))

# Command-line mode with setup
python3 -m timeit -s "L = [1,2,3,4,5]" "M = [x + 1 for x in L]"
```

#### Multiline Statements
```python
# API mode - use \n and \t for multiline
stmt = 'L = [1, 2, 3, 4, 5]\nfor i in range(len(L)):\n\tL[i] += 1'
min(timeit.repeat(stmt=stmt, number=100_000, repeat=5))

# Command-line mode - separate arguments for each line
python3 -m timeit "L = [1,2,3,4,5]" "i=0" "while i < len(L):" \
                  "    L[i] += 1" "    i += 1"
```

### Comparative Results
PyPy vs CPython using timeit:
```
# CPython 3.12
min(timeit.repeat('[x ** 2 for x in range(1000)]', number=1000, repeat=5))
# Result: 0.0519 seconds

# PyPy 7.3  
min(timeit.repeat('[x ** 2 for x in range(1000)]', number=1000, repeat=5))
# Result: 0.0025 seconds (20X faster!)
```

## Automated Benchmarking

### Benchmark Module (pybench.py)
```python
import sys, os, time, timeit

def runner(stmts, pythons=None, tracecmd=False):
    """
    stmts: [(number?, repeat?, stmt-string)]
    pythons: None=host python only, or [python-executable-paths]
    """
    for (number, repeat, stmt) in stmts:
        number = number or 1000
        repeat = repeat or 5
        
        if not pythons:
            # Single Python: API call
            best = min(timeit.repeat(stmt=stmt, number=number, repeat=repeat))
            print(f'{best:.4f}  {stmt[:70]!r}')
        else:
            # Multiple Pythons: command line
            for python in pythons:
                # Format for command line usage
                stmt = stmt.replace('"', "'")
                stmt = stmt.replace('\t', ' ' * 4)
                lines = stmt.split('\n')
                args = ' '.join(f'"{line}"' for line in lines)
                
                oscmd = f'{python} -m timeit -n {number} -r {repeat} {args}'
                print(oscmd if tracecmd else python)
                print('\t' + os.popen(oscmd).read().rstrip())
```

### Test Script Example
```python
import pybench, sys

pythons = [
    '/Library/Frameworks/Python.framework/Versions/3.12/bin/python3',
    '/Users/me/Downloads/pypy3.10-v7.3.16-macos_x86_64/bin/pypy3',
]

stmts = [
    (0, 0, '[x ** 2 for x in range(1000)]'),
    (0, 0, 'res=[]\nfor x in range(1000): res.append(x ** 2)'),
    (0, 0, 'list(map(lambda x: x ** 2, range(1000)))'),
    (0, 0, 'list(x ** 2 for x in range(1000))'),
]

tracecmd = '-t' in sys.argv
pythons = pythons if '-a' in sys.argv else None
pybench.runner(stmts, pythons, tracecmd)
```

### Usage Modes
```bash
# Test current Python only
python3 pybench_tests.py

# Test all listed Pythons
python3 pybench_tests.py -a

# Test all Pythons with command tracing
python3 pybench_tests.py -a -t
```

## Set and Dictionary Performance

### Test Results (CPython 3.12)
```python
# Sets
'{x ** 2 for x in range(1000)}'          # 0.0746 - Set comprehension
'set(x ** 2 for x in range(1000))'       # 0.0947 - Generator to set (slower)
's=set()\nfor x in range(1000): s.add(x ** 2)'  # 0.0834 - Loop

# Dictionaries  
'{x: x ** 2 for x in range(1000)}'       # 0.0745 - Dict comprehension
'dict((x, x ** 2) for x in range(1000))' # 0.1174 - Generator to dict (slower)
'd={}\nfor x in range(1000): d[x] = x ** 2'     # 0.0754 - Loop
```

**Key Finding:** Passing generators to type constructors is significantly slower than comprehensions or loops.

## Common Function Gotchas

### 1. Local Names Detected Statically
```python
# Problem: This will fail
X = 99
def selector():
    print(X)          # Error: X is local but not yet assigned
    X = 88            # This makes X local everywhere in function

# Solutions:
def selector():
    global X          # Declare X as global
    print(X)
    X = 88

# Or use qualified access
def selector():
    import __main__
    print(__main__.X)  # Access global version
    X = 88             # Create local version
```

### 2. Mutable Default Arguments
```python
# Problem: Default retains state between calls
def saver(x=[]):      # Same list object used each time
    x.append(1)
    print(x)
    
saver()  # [1]
saver()  # [1, 1] - List grows!

# Solution: Use None as default
def saver(x=None):
    if x is None:
        x = []        # New list each time
    x.append(1)
    print(x)
```

### 3. Functions Without Explicit Returns
```python
def proc(x):
    print(x)          # No return statement

result = proc('test')  # result is None
print(result)         # None

# Common mistake with list methods
mylist = [1, 2, 3]
mylist = mylist.append(4)  # append returns None!
print(mylist)             # None (list is lost)
```

### 4. Enclosing Scopes and Loop Variables
```python
# Problem: All functions reference same loop variable
funcs = []
for i in range(3):
    funcs.append(lambda: i)  # All reference final value of i

[f() for f in funcs]  # [2, 2, 2] - all return 2!

# Solution: Use default arguments to capture values
funcs = []
for i in range(3):
    funcs.append(lambda x=i: x)  # Capture current i value

[f() for f in funcs]  # [0, 1, 2] - correct!
```

## Best Practices Summary

### Performance Optimization
1. **Don't optimize prematurely** - Write for readability first
2. **Profile before optimizing** - Use actual measurements
3. **Test on your target platform** - Results vary by Python version and hardware
4. **Use appropriate tools** - Built-in functions often fastest for simple operations

### Benchmarking Guidelines
1. **Use best-of timing** to filter out system fluctuations
2. **Run sufficient iterations** for meaningful measurements
3. **Verify results are identical** across different implementations
4. **Consider both absolute and relative performance**
5. **Test with realistic data sizes** and usage patterns

### Timer Selection
- **Custom timers**: Good for learning and specific needs
- **timeit module**: Standard library option with more features
- **Profile modules**: For full-program analysis
- **Third-party tools**: Like pyperf for advanced benchmarking

### Key Takeaways
1. **List comprehensions** are generally fast and readable
2. **map()** excels with built-in functions, struggles with user functions
3. **Generators** are memory-efficient but slower for immediate results
4. **PyPy** can be dramatically faster than CPython for numeric code
5. **Function calls are expensive** - minimize in performance-critical code