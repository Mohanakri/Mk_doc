# Developing on EKS - Chapter 19 Notes

## Overview

This chapter covers tools and techniques for efficient deployment and testing of EKS clusters and workloads using automation and CI/CD, targeting developers and DevOps engineers.

## Topics Covered

- Different IT personas and cloud operating models
- Using Cloud9 as an integrated development environment
- Building clusters with EKS Blueprints and Terraform
- Using CodePipeline and CodeBuild for cluster automation
- Using Argo CD, Crossplane, and GitOps for workload deployment

## Technical Requirements

- Familiarity with YAML, AWS IAM, and EKS architecture
- Network connectivity to EKS cluster API endpoint
- AWS CLI, Docker, and kubectl binaries with administrator access

## IT Personas and Cloud Operating Model

### Functional Groups

1. **Application Engineers** - Build applications
2. **Application Operations** - Operate and support applications
3. **Platform Engineers** - Build middleware, networks, databases
4. **Platform Operations** - Operate infrastructure and middleware

### DevOps Model

- Combines Application Engineering and Application Operations
- Mantra: "you build it, you run it"
- May include platform engineering responsibilities

### Platform Engineering Teams

- Recent trend in organizations
- Support infrastructure used by developers/DevOps engineers
- Mantra: "you code and test and we'll do all the rest"
- Handle EKS, databases, messaging, and APIs

### Terminology Used

- **DevOps Engineers**: Application engineering/operations roles
- **Platform Engineers**: EKS cluster and support infrastructure roles

## Using Cloud9 as IDE

### Advantages

- Runs on EC2 inside your account
- Can communicate with private resources (private EKS clusters)
- Uses AWS Systems Manager Session Manager (IAM permissions only)
- EC2 instance roles with auto-refreshing credentials
- Integrated AWS toolkit for S3 and Lambda
- Docker container support with HTTP preview (ports 8080, 8081, 8082)
- Amazon CodeWhisperer integration for ML-powered code generation

### Creating Cloud9 Instance

```hcl
data "aws_region" "current" {}

locals {
  myuser_arn = "arn:aws:sts::123:myuser"
}

resource "aws_cloud9_environment_ec2" "k8sdev" {
  name            = "k8sdev"
  instance_type   = "t3.medium"
  connection_type = "CONNECT_SSM"
  description     = "cloud9 K8s development environment"
  subnet_id       = "subnet-123"
  owner_arn       = local.myuser_arn
}

data "aws_instance" "cloud9_instance" {
  filter {
    name   = "tag:aws:cloud9:environment"
    values = [aws_cloud9_environment_ec2.k8sdev.id]
  }
}
```

### Configuration Steps

1. **Create IAM Role with AdministratorAccess**
   - Trust relationship with `ec2.amazonaws.com`
   - Disable AWS managed temporary credentials
   - Attach role to EC2 instance

2. **Verify Role Attachment**
   ```bash
   aws sts get-caller-identity
   ```

3. **Install Required Tools**
   ```bash
   # Download and run setup script
   wget https://jiwony-seoul-public.s3.ap-northeast-2.amazonaws.com/cloud9-prereq.sh
   sh cloud9-prereq.sh
   
   # Install Terraform
   sudo yum install -y yum-utils
   sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
   sudo yum -y install terraform
   
   # Install eksctl
   ARCH=amd64
   PLATFORM=$(uname -s)_$ARCH
   curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
   tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
   sudo mv /tmp/eksctl /usr/local/bin
   
   # Configure default region
   aws configure
   ```

## Building Clusters with EKS Blueprints and Terraform

### EKS Blueprints Overview

- Provides opinionated cluster builds with operational software pre-deployed
- Simplifies platform/DevOps engineer tasks
- Built using AWS CDK (now also available for Terraform)
- Enables repeatable cluster builds for different environments/teams

### Development Life Cycle Approach

1. **Download** blueprints code
2. **Version** in source control
3. **Customize** for requirements
4. **Deploy** and test
5. **Iterate** and improve

### Setting Up Repository

```bash
# Create CodeCommit repository
aws codecommit create-repository \
  --repository-name cluster-tf \
  --repository-description "repository for TF Blueprint" \
  --tags Team=devops \
  --region eu-central-1

# Clone and create branch
git clone https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf
cd cluster-tf
git checkout -b initial
```

### Base Configuration Files

#### providers.tf
```hcl
terraform {
  required_version = ">= 1.0.1"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.47"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.10"
    }
    helm = {
      source  = "hashicorp/helm"
      version = ">= 2.4.1"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = ">= 1.14"
    }
  }
}
```

#### data.tf
```hcl
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
data "aws_availability_zones" "available" {
  state = "available"
}
```

#### locals.tf
```hcl
locals {
  name            = basename(path.cwd)
  region          = data.aws_region.current.name
  cluster_version = "1.24"
  vpc_cidr        = "172.31.0.0/16"
  azs             = slice(data.aws_availability_zones.available.names, 0, 3)
  node_group_name = "mgmt-nodegroup"
  
  tags = {
    Blueprint  = local.name
    GithubRepo = "github.com/aws-ia/terraform-aws-eks-blueprints"
  }
}
```

### Creating EKS VPC

#### vpc.tf
```hcl
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.16.0"
  
  name = local.name
  cidr = local.vpc_cidr
  azs  = local.azs
  
  public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k)]
  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 10)]
  
  enable_nat_gateway   = true
  create_igw           = true
  enable_dns_hostnames = true
  single_nat_gateway   = true
  
  # Network ACL and Route Table Management
  manage_default_network_acl    = true
  default_network_acl_tags      = { Name = "${local.name}-default" }
  manage_default_route_table    = true
  default_route_table_tags      = { Name = "${local.name}-default" }
  manage_default_security_group = true
  default_security_group_tags   = { Name = "${local.name}-default" }
  
  # EKS Required Tags
  public_subnet_tags = {
    "kubernetes.io/cluster/${local.name}" = "shared"
    "kubernetes.io/role/elb"              = "1"
  }
  
  private_subnet_tags = {
    "kubernetes.io/cluster/${local.name}" = "shared"
    "kubernetes.io/role/internal-elb"     = "1"
  }
  
  tags = local.tags
}
```

#### outputs.tf
```hcl
output "vpc_id" {
  description = "The id of the new VPC"
  value       = module.vpc.vpc_id
}
```

### Creating EKS Cluster

#### main.tf
```hcl
provider "aws" {
  region = "us-east-1"
  alias  = "virginia"
}

provider "kubernetes" {
  host                   = module.eks_blueprints.eks_cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.this.token
}

provider "helm" {
  kubernetes {
    host                   = module.eks_blueprints.eks_cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)
    token                  = data.aws_eks_cluster_auth.this.token
  }
}

provider "kubectl" {
  apply_retry_count      = 10
  host                   = module.eks_blueprints.eks_cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)
  load_config_file       = false
  token                  = data.aws_eks_cluster_auth.this.token
}

module "eks_blueprints" {
  source = "github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.31.0"
  
  cluster_name       = local.name
  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnets
  cluster_version    = local.cluster_version
  
  managed_node_groups = {
    mg_5 = {
      node_group_name = local.node_group_name
      instance_types  = ["m5.large"]
      subnet_ids      = module.vpc.private_subnets
    }
  }
  
  tags = local.tags
}
```

#### eks-data.tf
```hcl
data "aws_eks_cluster" "cluster" {
  name = module.eks_blueprints.eks_cluster_id
}

data "aws_eks_cluster_auth" "this" {
  name = module.eks_blueprints.eks_cluster_id
}

# ECR Public authentication in us-east-1
data "aws_ecrpublic_authorization_token" "token" {
  provider = aws.virginia
}
```

#### eks-output.tf
```hcl
output "configure_kubectl" {
  description = "run the following command to update your kubeconfig"
  value       = module.eks_blueprints.configure_kubectl
}
```

### Adding Users and Teams

#### Role Mapping
```hcl
map_roles = [
  {
    rolearn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/Admin"
    username = "admin-role"
    groups   = ["system:masters"]
  }
]
```

#### Team Configuration (locals-team.tf)
```hcl
locals {
  platform_admins = ["arn:aws:iam::123:role/plat1"]
  app_team_1      = ["arn:aws:iam::123:role/dev1"]
}
```

#### Platform Teams
```hcl
platform_teams = {
  admin = {
    users = local.platform_admins
  }
}
```

#### Application Teams
```hcl
application_teams = {
  alpha = {
    "labels" = {
      "appName"     = "alpha",
      "projectName" = "project-alpha",
      "environment" = "dev"
    }
    "quota" = {
      "pods"     = "15",
      "services" = "10"
    }
    users = [local.app_team_alpha]
  }
}
```

### Adding Blueprints Add-ons

#### locals-blueprints.tf
```hcl
locals {
  addon_application = {
    path               = "chart"
    repo_url           = "https://github.com/aws-samples/eks-blueprints-add-ons.git"
    add_on_application = true
  }
}
```

#### blueprints.tf
```hcl
module "kubernetes_addons" {
  source = "github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.31.0/modules/kubernetes-addons"
  
  eks_cluster_id        = module.eks_blueprints.eks_cluster_id
  enable_argocd         = true
  argocd_manage_add_ons = true
  
  argocd_applications = {
    addons = local.addon_application
  }
  
  argocd_helm_config = {
    set = [{
      name  = "server.service.type"
      value = "LoadBalancer"
    }]
  }
  
  # Add-ons
  enable_aws_load_balancer_controller  = true
  enable_amazon_eks_aws_ebs_csi_driver = true
  enable_aws_for_fluentbit             = true
  enable_metrics_server                = true
  enable_crossplane                    = true
  enable_karpenter                     = true
}
```

### Accessing ArgoCD
```bash
# Get ArgoCD server URL
export ARGOCD_SERVER=`kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`
echo https://$ARGOCD_SERVER

# Get admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```

## Using CodePipeline and CodeBuild

### Terraform State Management

#### Backend Configuration (providers.tf)
```hcl
terraform {
  # ... existing configuration
  backend "s3" {}
}
```

### BuildSpec Configuration

#### buildspec.yml
```yaml
version: 0.2

env:
  exported-variables:
    - BuildID
    - BuildTag

phases:
  install:
    commands:
      - yum update -y
      - yum install -y yum-utils
      - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
      - yum -y install terraform jq
      - terraform version

  pre_build:
    commands:
      - echo creating S3 backend for bucket ${TFSTATE_BUCKET} region ${TFSTATE_REGION} prefix ${TFSTATE_KEY}
      - cd "$CODEBUILD_SRC_DIR"
      - terraform init -input=false -backend-config="bucket=${TFSTATE_BUCKET}" -backend-config="key=${TFSTATE_KEY}" -backend-config="region=${TFSTATE_REGION}"
      - terraform validate

  build:
    commands:
      - echo running command terraform ${TF_ACTION}
      - cd "$CODEBUILD_SRC_DIR"
      - terraform ${TF_ACTION} -input=false
```

### CodeBuild Project Setup

#### Required Environment Variables
- `TFSTATE_BUCKET`: S3 bucket name for state storage
- `TF_ACTION`: Terraform action (apply -auto-approve, destroy, plan)
- `TFSTATE_KEY`: State file path/prefix
- `TFSTATE_REGION`: AWS region for state bucket

#### Service Role Configuration
- CodeBuild service role needs IAM permissions for Terraform operations
- Add service role to cluster's `map_roles` configuration

### CodePipeline Configuration

#### Pipeline Stages
1. **Source Stage**: CodeCommit repository with branch monitoring
2. **Build Stage**: CodeBuild project execution

#### Trigger Mechanism
- CloudWatch Events detect commits
- Automatic pipeline execution on code changes

## Using ArgoCD, Crossplane, and GitOps

### GitOps Architecture

GitOps enables continuous deployment focusing on:
- Self-service developer experience
- Infrastructure as code alongside application code
- Git repository as single source of truth

### Key Components

- **ArgoCD**: Deployment tool polling Git repositories
- **Crossplane**: Kubernetes resources for AWS infrastructure
- **Kustomize**: Kubernetes manifest customization

### Application Repository Setup

#### Directory Structure
```
myapp/
├── base/
│   ├── namespace.yaml
│   ├── deployment.yaml
│   └── kustomization.yaml
└── overlays/
    ├── non-production/
    │   ├── kustomization.yaml
    │   └── deployment.yaml
    └── production/
        ├── kustomization.yaml
        └── deployment.yaml
```

#### Base Manifests

**namespace.yaml**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: app
```

**deployment.yaml**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
  namespace: app
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2
          resources:
            requests:
              memory: 1Gi
```

**base/kustomization.yaml**
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - namespace.yaml
  - deployment.yaml
```

#### Environment Overlays

**overlays/non-production/kustomization.yaml**
```yaml
resources:
- ../../base
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: non-production
namePrefix: non-production-
patches:
- path: deployment.yaml
```

**overlays/non-production/deployment.yaml**
```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: inflate
          resources:
            limits:
              memory: 1250Mi
            requests:
              memory: 1250Mi
```

### ArgoCD Application Setup

#### Prerequisites
- SSH credentials for CodeCommit access
- ArgoCD CLI installation

#### Configuration Commands
```bash
# Install ArgoCD CLI
sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd

# Set environment variables
ARGOCD_SERVER=$(kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname')
ARGOCD_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
GITOPS_IAM_SSH_KEY_ID=APKARDV7UN6242ZX
AWS_DEFAULT_REGION=eu-central-1
APP_REPO_NAME=myapp
GITOPS_REPO_URL=ssh://${GITOPS_IAM_SSH_KEY_ID}@git-codecommit.${AWS_DEFAULT_REGION}.amazonaws.com/v1/repos/${APP_REPO_NAME}

# Login to ArgoCD
argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PWD --insecure

# Add repository
argocd repo add $(cat ./argocd_repo_url) --ssh-private-key-path ${HOME}/.ssh/argocd --insecure-ignore-host-key --upsert --name myapp

# Create application
argocd app create myapp \
  --repo $(cat ./argocd_repo_url) \
  --path overlays/non-production \
  --dest-server https://kubernetes.default.svc \
  --sync-policy automated \
  --self-heal \
  --auto-prune
```

### Crossplane for AWS Infrastructure

#### Installation
```bash
# Install Helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

# Install Crossplane
helm repo add crossplane-stable https://charts.crossplane.io/stable
helm install crossplane --create-namespace --namespace crossplane-system crossplane-stable/crossplane
```

#### AWS Provider Setup

**IRSA Role Creation**
```bash
account_id=$(aws sts get-caller-identity --query "Account" --output text)
oidc_provider=$(aws eks describe-cluster --name src --region $AWS_DEFAULT_REGION --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")

# Create trust policy
cat > trust.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::${account_id}:oidc-provider/${oidc_provider}"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringLike": {
          "${oidc_provider}:sub": "system:serviceaccount:crossplane-system:provider-aws-*"
        }
      }
    }
  ]
}
EOF

# Create role and attach policy
aws iam create-role --role-name bespoke-crossplane --assume-role-policy-document file://trust.json --description "Crossplane IRSA role"
aws iam attach-role-policy --role-name bespoke-crossplane --policy-arn=arn:aws:iam::aws:policy/AdministratorAccess
```

**Provider Configuration**
```yaml
apiVersion: pkg.crossplane.io/v1alpha1
kind: ControllerConfig
metadata:
  name: aws-config
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::112233:role/bespoke-crossplane
spec:
  podSecurityContext:
    fsGroup: 2000
  args:
    - --debug
---
apiVersion: pkg.crossplane.io/v1
kind: Provider
metadata:
  name: provider-aws
spec:
  package: xpkg.upbound.io/upbound/provider-aws:v0.27.0
  controllerConfigRef:
    name: aws-config
```

**Provider Config**
```yaml
apiVersion: aws.upbound.io/v1beta1
kind: ProviderConfig
metadata:
  name: provider-aws
spec:
  credentials:
    source: IRSA
```

#### Creating AWS Resources

**S3 Bucket Example**
```yaml
apiVersion: s3.aws.upbound.io/v1beta1
kind: Bucket
metadata:
  name: myapp-crossplane-bucket637678
spec:
  forProvider:
    region: eu-central-1
  providerConfigRef:
    name: provider-aws
```

## Key Deployment Commands

### Terraform Workflow
```bash
# Initialize and validate
terraform init
terraform validate
terraform plan

# Apply changes
terraform apply --auto-approve

# Update kubectl config
aws eks --region eu-central-1 update-kubeconfig --name cluster-tf

# Verify cluster
kubectl get nodes
```

### Git Workflow
```bash
# Commit changes
git add .
git commit -m "deployment message"
git push
```

### ArgoCD Management
```bash
# List applications
argocd app list

# Sync application
argocd app sync myapp

# Get application status
argocd app get myapp
```

## Best Practices

### Security
- Use least privilege IAM policies instead of AdministratorAccess
- Implement proper RBAC for cluster access
- Use IRSA for service account authentication
- Store sensitive data in AWS Secrets Manager

### State Management
- Use S3 backend for Terraform state
- Enable state locking with DynamoDB
- Implement state file encryption
- Use separate state files per environment

### GitOps
- Implement branch protection rules
- Use pull request workflows for changes
- Separate application and infrastructure repositories
- Implement automated testing in CI/CD pipelines

### Monitoring and Logging
- Enable debug logging during setup (disable in production)
- Monitor ArgoCD application health
- Set up alerts for deployment failures
- Use structured logging for troubleshooting

## Troubleshooting

### Common Issues
- **Terraform state conflicts**: Use proper locking mechanisms
- **ArgoCD sync failures**: Check repository access and SSH keys
- **Crossplane provider issues**: Verify IRSA configuration and permissions
- **Network connectivity**: Ensure proper subnet tagging for load balancers

### Debugging Commands
```bash
# Check Terraform state
terraform show

# ArgoCD logs
kubectl logs -n argocd deployment/argo-cd-argocd-server

# Crossplane provider logs
kubectl logs -n crossplane-system deployment/provider-aws

# Check resource status
kubectl describe <resource-type> <resource-name>
```

## Summary

This chapter covered comprehensive EKS development practices including:

- Setting up secure development environments with Cloud9
- Using EKS Blueprints for opinionated cluster deployment
- Implementing CI/CD with CodePipeline and CodeBuild
- Adopting GitOps practices with ArgoCD and Kustomize
- Managing infrastructure as code with Crossplane

The approaches enable efficient, automated, and scalable EKS development workflows suitable for different organizational models and team structures.