# Troubleshooting Common Issues - Chapter 20 Notes

## Overview

This chapter covers common troubleshooting techniques and tools for identifying and resolving issues in Amazon Elastic Kubernetes Service (EKS). Understanding how to diagnose problems is essential for effectively operating EKS clusters.

## Topics Covered

- Common Kubernetes tools and techniques for troubleshooting EKS
- Common cluster access problems
- Common Node/compute problems
- Common Pod networking problems
- Common workload problems

## Technical Requirements

- Familiarity with YAML, AWS IAM, and EKS architecture
- Network connectivity to EKS cluster API endpoint
- AWS CLI, Docker, and kubectl binaries with administrator access

## General Troubleshooting Approach

### Problem-Solving Process

1. **Understand the Problem**
   - Differentiate between symptoms and root cause
   - Use iterative testing and observation
   - Focus on actual problems, disregard false positives

2. **Develop Solutions**
   - Understand how to mitigate, solve, or work around issues
   - Create strategies for problems that can't be immediately resolved

3. **Implement Resolution**
   - May require cluster updates, application code changes, or both
   - Consider impact on users/customers and number of clusters

### Troubleshooting Checklist

#### 1. What Has Changed?
- New deployments?
- Cluster or add-on upgrades?
- AWS/on-premises infrastructure changes?
- Maintain change records (especially in production)

#### 2. Investigate the Impact
- Analyze symptoms: slowdown vs. complete failure
- Determine scope: one namespace, cluster, or VPC?
- Narrow down to root cause through hypothesis testing

#### 3. Plan Fix or Mitigation
- Assess impact of potential fixes
- Plan changes with team and stakeholders
- Test in non-production environment when possible

## Common EKS Troubleshooting Tools

### What Has Changed Phase

| Tool | Description | Use Case |
|------|-------------|----------|
| **AWS CloudTrail** | Logs of every API call in AWS accounts | Track EKS API calls, load balancer changes, IAM modifications |
| **AWS CloudWatch** | Dashboards and logs for control/data planes | Monitor cluster metrics and logs |
| **Grafana Loki** | Open source log aggregator | Centralized log analysis |
| **kubectl diff** | Compare local manifest with running configuration | Identify configuration changes |

### Investigating Impact Phase

| Tool | Description | Use Case |
|------|-------------|----------|
| **Linux CLI Tools** | dig, ping, tcpdump, etc. | Determine scope and network impact |
| **kubectl** | Primary Kubernetes CLI | `describe`, `get events`, `logs`, `top` commands |
| **Ktop** | CLI for cluster management | Interactive cluster monitoring |
| **k9s** | Terminal-based UI for Kubernetes | Visual cluster management |
| **AWS Log Collector** | AWS support script for log collection | Collect OS and K8s logs for support cases |

### Advanced Debugging Tools

- **kubectl debug**: Inject troubleshooting container into running pods (K8s 1.25+)

### AWS Support Services

#### AWS Premium Support
- **Plans**: Developer, Business, Enterprise
- **Access**: Email, phone, chat support
- **Features**: 24/7/365 technical support, access to AWS expertise
- **Support Console**: AWS Support case management

#### AWS Support Knowledge Center
- Centralized resource for operational issues
- Most frequent questions and solutions
- Amazon EKS specific articles with video tutorials
- URL: https://aws.amazon.com/premiumsupport/knowledge-center/

## Common Cluster Access Problems

### Symptom: Cannot Access Cluster with kubectl

#### Error: No Credentials Configured
```bash
$ kubectl get node
Unable to locate credentials. You can configure credentials by running "aws configure".
```

**Root Cause**: No AWS credentials in environment variables or `~/.aws` directory

**Solution**: Configure AWS credentials
```bash
aws configure
# or
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=your_region
```

#### Error: Unauthorized Access
```bash
$ kubectl get node
error: You must be logged in to the server (Unauthorized)
```

**Root Causes**:
- Insufficient IAM privileges
- Missing Kubernetes RBAC permissions
- User/role not in aws-auth ConfigMap

**Solutions**:
- Verify IAM permissions for EKS access
- Check aws-auth ConfigMap configuration
- Ensure proper RBAC setup

#### Error: Connection Timeout
```bash
$ kubectl get node
Unable to connect to the server: dial tcp 3.69.90.98:443: i/o timeout
```

**Root Causes**:
- Network connectivity issues with private EKS cluster
- IP whitelist restrictions
- Security group misconfigurations
- IAM role access issues

**Solutions**:
- Check network connectivity to cluster endpoint
- Verify security group rules
- Review IP whitelist configurations
- Validate IAM role permissions

## Common Node/Compute Problems

### Symptom: Nodes Cannot Join Cluster

#### Error: Nodes in NotReady/Unknown State
```bash
$ kubectl get node
NAME                          STATUS     ROLES    AGE    VERSION
ip-172-31-10-50.x.internal    NotReady   <none>   7d4h   v1.24.13-eks-0a21954
ip-172-31-11-89.x.internal    Unknown    <none>   7d4h   v1.24.13-eks-0a21954
```

**Root Causes and Solutions**:

1. **Self-managed nodes**: bootstrap.sh script not executed
   - Ensure EC2 user data runs bootstrap script properly

2. **IP whitelist issues**: Node/NAT gateway IPs not whitelisted
   - Add public IPs of nodes or NAT gateway to whitelist

3. **Security group issues**: Worker nodes can't communicate with cluster
   - Configure worker node security group with cluster security group

#### Error: Node Not Found in kubelet Logs
```bash
Dec 23 17:42:36 ... kubelet[92039]: E1223 17:42:36.551307 92039 kubelet.go:2240] node "XXXXXXXXX" not found
```

**Root Cause**: VPC DNS configuration issues

**Solution**: Enable DNS hostnames and resolution in VPC
```bash
aws ec2 modify-vpc-attribute --vpc-id vpc-12345678 --enable-dns-hostnames
aws ec2 modify-vpc-attribute --vpc-id vpc-12345678 --enable-dns-support
```

#### Error: Kubernetes Cluster Unreachable
```bash
Error: Kubernetes cluster unreachable: Get "https://XXXXXXXXXX.gr7.eu-central-1.eks.amazonaws.com/version?timeout=32s": dial tcp <API_SERVER_IP>:443: i/o timeout
```

**Root Cause**: Networking misconfiguration

**Solutions**:
- Check security groups
- Verify NACLs
- Confirm NAT gateway configuration
- Validate VPC endpoints

#### Error: Unauthorized Node Status Update
```bash
Jan 01 12:23:01 XXXXXXXXX kubelet[4445]: E1012 12:23:01.369732 4445 kubelet_node_status.go:377] Error updating node status, will retry: error getting node "XXXXXXXXX": Unauthorized
```

**Root Cause**: IAM authentication issue

**Solution**: Add node's IAM role to aws-auth ConfigMap
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::ACCOUNT:role/NodeInstanceRole
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
```

#### Error: Disk Pressure
```bash
Conditions:
Type           Status  LastHeartbeatTime                 LastTransitionTime                Reason                     Message
MemoryPressure False   Tue, 12 Jul 2022 03:10:33 +0000  Wed, 29 Jun 2022 13:21:17 +0000  KubeletHasSufficientMemory kubelet has sufficient memory available
DiskPressure   True    Tue, 12 Jul 2022 03:10:33 +0000  Wed, 06 Jul 2022 19:46:54 +0000  KubeletHasDiskPressure     kubelet has disk pressure
```

**Root Cause**: Low disk space (often due to excessive logging)

**Solutions**:
- Clean up disk space
- Configure log rotation
- Increase node storage
- Monitor disk usage

## Common Pod Networking Problems

### Connection Timeout Errors

#### Error: Pod-to-Pod Connection Timeout
```bash
* connect to 172.16.24.25 port 8080 failed: Connection timed out
* Closing connection 0
curl: (7) Failed to connect to 172.16.24.25 port 8080 failed: Connection timed out
```

**Root Cause**: Security group misconfigurations

**Solutions**:
- Check worker security group allows required ports
- Verify IP CIDR ranges are correct
- Ensure security group allows itself as source
- Check NACLs allow ephemeral ports outbound

#### Error: DNS Resolution Issues
```bash
Error: RequestError: send request failed caused by: Post dial tcp: i/o timeout
```

**Root Cause**: DNS configuration problems

**Solutions**:
- Verify CoreDNS is running: `kubectl get pods -n kube-system -l k8s-app=kube-dns`
- Check clusterDNS configuration
- Ensure VPC has DNS resolution and hostnames enabled

#### Error: General Connectivity Issues
```bash
Error: RequestError: send request failed caused by: Post dial tcp 172.16.24.25:443: i/o timeout
```

**Root Causes**:
- Network policy restrictions
- Security group misconfigurations
- Route table issues

### IP Address Allocation Problems

#### Error: Failed to Assign IP Address
```bash
Failed create pod mypod: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod network: add cmd: failed to assign an IP address to container
```

**Root Causes**:
- VPC has no free IP addresses
- EC2 instance type IP limit reached (without prefix addressing)

**Solutions**:
- Enable prefix addressing for VPC CNI
- Use larger subnets
- Scale to larger instance types
- Implement IP address management

### CNI and DNS Issues

#### Error: CNI and CoreDNS Failures
```bash
NAME           READY   STATUS             RESTARTS   AGE
aws-node-bbwpq 0/1     CrashLoopBackOff   12         51m
aws-node-nw7v8 0/1     CrashLoopBackOff   12         51m
coredns-12     0/1     Pending            0          54m
coredns-13     0/1     Pending            0          54m
```

**Root Cause**: VPC or security group misconfigurations affecting CNI/DNS

**Solutions**:
- Check VPC CNI daemonset logs
- Verify worker node security groups
- Ensure proper IAM roles for CNI

### Load Balancer Configuration Issues

#### Error: Subnet Resolution Failure
```bash
"msg"="Reconciler error" "error"="failed to build LoadBalancer configuration due to retrieval of subnets failed to resolve 2 qualified subnets."
```

**Root Cause**: Subnets not properly tagged for load balancer discovery

**Solutions**:
- Tag public subnets for external load balancers:
  ```
  kubernetes.io/role/elb = 1
  ```
- Tag private subnets for internal load balancers:
  ```
  kubernetes.io/role/internal-elb = 1
  ```

## Common Workload Problems

### Memory and Resource Issues

#### Error: Out of Memory Killed (OOMKilled)
```bash
State:          Running
Started:        Sun, 16 Feb 2020 10:20:09 +0000
Last State:     Terminated
Reason:         OOMKilled
```

**Root Cause**: Pod exceeded memory limits

**Solutions**:
- Increase memory limits in pod specification:
  ```yaml
  resources:
    limits:
      memory: "512Mi"
    requests:
      memory: "256Mi"
  ```
- Analyze application memory usage
- Implement memory profiling

### Pod Scheduling Issues

#### Error: CrashLoopBackOff
```bash
NAME                    READY   STATUS             RESTARTS   AGE
myDeployment1-123...    1/1     Running            1          17m
myDeployment1-234...    0/1     CrashLoopBackOff   2          1m
```

**Root Causes**:
- Application startup failures
- Incorrect container configuration
- Missing dependencies

**Diagnostic Commands**:
```bash
kubectl logs pod-name --previous
kubectl describe pod pod-name
kubectl get events --sort-by=.metadata.creationTimestamp
```

#### Error: Insufficient CPU
```bash
Warning  FailedScheduling  22s (x14 over 13m)  default-scheduler  0/3 nodes are available: 3 Insufficient cpu.
```

**Root Cause**: Not enough CPU resources available

**Solutions**:
- Scale up cluster (add more nodes)
- Reduce CPU requests
- Use cluster autoscaler
- Review resource requests vs. limits

#### Error: Insufficient Memory
```bash
Warning  FailedScheduling  80s (x14 over 15m)  default-scheduler  0/3 nodes are available: 3 Insufficient memory.
```

**Root Cause**: Not enough memory resources available

**Solutions**:
- Scale up cluster (add more nodes)
- Reduce memory requests
- Use cluster autoscaler
- Optimize application memory usage

### Image and Container Issues

#### Error: Image Pull Failure
```bash
State:          Waiting
Reason:         ErrImagePull
```

**Root Causes**:
- Incorrect image name/tag
- Image not available in registry
- Private registry authentication issues

**Solutions**:
- Verify image name and tag
- Check registry accessibility
- Configure image pull secrets for private registries:
  ```bash
  kubectl create secret docker-registry regcred \
    --docker-server=your-registry-server \
    --docker-username=your-name \
    --docker-password=your-password \
    --docker-email=your-email
  ```

### Node Affinity and Tolerations

#### Error: Taint Toleration Issues
```bash
Warning  FailedScheduling  77s (x3 over 79s)  default-scheduler  0/1 nodes are available: 1 node(s) had taint {node-type: high-memory}, that the pod didn't tolerate.
```

**Root Cause**: Pod doesn't have matching tolerations for node taints

**Solution**: Add tolerations to pod specification:
```yaml
tolerations:
- key: "node-type"
  operator: "Equal"
  value: "high-memory"
  effect: "NoSchedule"
```

## Essential Diagnostic Commands

### Basic Cluster Information
```bash
# Cluster status
kubectl cluster-info
kubectl get nodes -o wide

# System pods status
kubectl get pods -n kube-system

# Recent events
kubectl get events --sort-by=.metadata.creationTimestamp
```

### Node Diagnostics
```bash
# Node details
kubectl describe node <node-name>

# Node resource usage (requires metrics server)
kubectl top nodes

# Node conditions
kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type
```

### Pod Diagnostics
```bash
# Pod details
kubectl describe pod <pod-name> -n <namespace>

# Pod logs
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace> --previous

# Pod resource usage
kubectl top pods -n <namespace>

# Execute commands in pod
kubectl exec -it <pod-name> -n <namespace> -- /bin/bash
```

### Network Diagnostics
```bash
# DNS resolution test
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup kubernetes.default

# Network connectivity test
kubectl run -it --rm debug --image=busybox --restart=Never -- ping <target-ip>

# Port connectivity test
kubectl run -it --rm debug --image=busybox --restart=Never -- telnet <host> <port>
```

### Service and Ingress Diagnostics
```bash
# Service details
kubectl describe service <service-name> -n <namespace>

# Endpoint information
kubectl get endpoints <service-name> -n <namespace>

# Ingress details
kubectl describe ingress <ingress-name> -n <namespace>
```

## Troubleshooting Workflows

### Pod Issues Workflow
1. **Check pod status**: `kubectl get pods`
2. **Describe pod**: `kubectl describe pod <pod-name>`
3. **Check logs**: `kubectl logs <pod-name>`
4. **Check events**: `kubectl get events`
5. **Verify resources**: `kubectl top pods`
6. **Check node status**: `kubectl get nodes`

### Network Issues Workflow
1. **Test DNS**: Run DNS resolution tests
2. **Check connectivity**: Test pod-to-pod communication
3. **Verify security groups**: Ensure ports are open
4. **Check CNI**: Verify aws-node daemonset status
5. **Validate service**: Test service endpoints

### Node Issues Workflow
1. **Check node status**: `kubectl get nodes`
2. **Describe node**: `kubectl describe node <node-name>`
3. **Check system logs**: Review kubelet logs
4. **Verify IAM**: Check node instance profile
5. **Network validation**: Test cluster API connectivity

## Best Practices for Troubleshooting

### Logging and Monitoring
- Enable comprehensive logging for applications
- Use structured logging formats
- Implement centralized log aggregation
- Set up monitoring and alerting
- Maintain dashboards for key metrics

### Documentation and Change Management
- Document known issues and solutions
- Maintain runbooks for common procedures
- Track changes in production environments
- Use version control for configurations
- Implement change approval processes

### Preventive Measures
- Regular cluster health checks
- Automated testing of deployments
- Resource quota management
- Network policy implementation
- Security scanning and updates

### Emergency Response
- Establish incident response procedures
- Define escalation paths
- Prepare rollback strategies
- Maintain communication channels
- Post-incident review processes

## AWS Support Integration

### When to Engage AWS Support
- Complex networking issues
- Control plane problems
- Performance degradation
- Security concerns
- Add-on compatibility issues

### Information to Provide
- Cluster details (name, region, version)
- Node information and logs
- Network configuration
- Error messages and timestamps
- Steps to reproduce the issue

### Log Collection
Use AWS Log Collector script:
```bash
curl -O https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/log-collector-script/linux/eks-log-collector.sh
sudo bash eks-log-collector.sh
```

## Summary

This chapter covered comprehensive troubleshooting approaches for EKS clusters including:

- **Systematic problem-solving methodology** with clear steps for diagnosis and resolution
- **Common tools and techniques** for identifying issues across different layers
- **Specific error patterns** with root causes and solutions for:
  - Cluster access problems
  - Node and compute issues
  - Pod networking problems
  - Workload deployment issues
- **Diagnostic commands and workflows** for efficient troubleshooting
- **Best practices** for prevention and incident response

Effective troubleshooting requires understanding the symptoms, identifying root causes, and applying appropriate solutions while considering the impact on users and systems. Regular practice with these tools and techniques will improve your ability to quickly resolve issues in production environments.

## Further Reading

- [Debugging Kubernetes Tools - CNCF Blog](https://www.cncf.io/blog/2022/09/15/10-critical-kubernetes-tools-and-how-to-debug-them/)
- [EKS Official Troubleshooting Guide](https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.html)
- [AWS Support Knowledge Center - EKS](https://aws.amazon.com/premiumsupport/knowledge-center/)
- [Kubernetes Troubleshooting Documentation](https://kubernetes.io/docs/tasks/debug-application-cluster/)