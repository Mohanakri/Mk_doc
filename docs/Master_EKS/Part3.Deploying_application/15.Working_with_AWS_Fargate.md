# AWS EKS IAM and Fargate Notes

## AWS EKS IAM and IRSA Notes

### Overview
This chapter covers how to grant IAM permissions to Kubernetes Pods running on Amazon EKS, focusing on IAM Roles for Service Accounts (IRSA) as a secure alternative to traditional EC2 instance profiles.

## Prerequisites
- Network connectivity to EKS cluster API endpoint
- AWS CLI, Docker, and kubectl installed
- Basic understanding of AWS IAM and EKS architecture
- Familiarity with YAML

## Traditional EC2 IAM Role Assignment

### How EC2 Instance Profiles Work
- **Instance Profile**: Container for IAM role attached to EC2 instance
- **Process**:
  1. EC2 instance created with assigned role
  2. AWS automatically creates instance profile
  3. Instance queries Instance Metadata Service (IMDS) at `169.254.169.254`
  4. Retrieves access credentials (AccessKeyId, SecretAccessKey, Token)

### Example Credential Response
```json
{
  "Code": "Success",
  "LastUpdated": "2022-04-26T16:39:16Z",
  "Type": "AWS-HMAC",
  "AccessKeyId": "ASIAIOSFODNN7EXAMPLE",
  "SecretAccessKey": "bPxRfiCYEXAMPLEKEY",
  "Token": "token",
  "Expiration": "2022-05-17T15:09:54Z"
}
```

### Checking Instance Profile
```bash
aws sts get-caller-identity
```

## Instance Metadata Service Version 2 (IMDSv2)

### Security Enhancements
- **Session-oriented requests** with token-based authentication
- **TTL hop count** restriction (default: 1)
- **Prevents Pod access** to host credentials (Pods increment hop count to 2)

### Configuring IMDSv2
```bash
aws ec2 modify-instance-metadata-options \
  --instance-id i-1122233 \
  --http-tokens required \
  --http-put-response-hop-limit 1
```

### Limitations
- Only works if Pod uses IMDSv2
- Pods using IMDSv1 can still retrieve host credentials
- **Best Practice**: Give worker nodes minimal permissions only

## IAM Roles for Service Accounts (IRSA)

### What IRSA Solves
- Assigns specific privileges to specific Pods
- Eliminates inheritance of worker node permissions
- Implements least-privilege principle
- Maps Kubernetes Service Accounts to IAM roles

### How IRSA Works
1. **Association**: Kubernetes Service Account mapped to IAM role
2. **API Call**: Pod uses Service Account credentials with IAM role annotation
3. **Token Exchange**: `AssumeRoleWithWebIdentity` call to AWS STS
4. **Credential Exchange**: Kubernetes credentials exchanged for AWS IAM credentials

## Implementing IRSA

### Step 1: Verify OIDC Provider
```bash
# Check if OIDC provider exists
aws eks describe-cluster --name myipv4cluster \
  --query "cluster.identity.oidc.issuer" --output text

# Create OIDC provider if needed
eksctl utils associate-iam-oidc-provider --cluster cluster_name --approve
```

### Step 2: Create IAM Policy
Example S3 access policy:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetAccountPublicAccessBlock",
        "s3:GetBucketAcl",
        "s3:GetBucketLocation",
        "s3:GetBucketPolicyStatus",
        "s3:GetBucketPublicAccessBlock",
        "s3:ListAccessPoints",
        "s3:ListAllMyBuckets"
      ],
      "Resource": "*"
    }
  ]
}
```

```bash
aws iam create-policy --policy-name bespoke-pod-policy \
  --policy-document file://s3-policy.json
```

### Step 3: Set Environment Variables
```bash
export account_id=$(aws sts get-caller-identity --query "Account" --output text)
export oidc_provider=$(aws eks describe-cluster --name myipv4cluster \
  --region eu-central-1 --query "cluster.identity.oidc.issuer" \
  --output text | sed -e "s/^https:\/\///")
export namespace=default
export service_account=s3-access
```

### Step 4: Create Trust Relationship
```bash
cat >trust-relationship.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::$account_id:oidc-provider/$oidc_provider"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "$oidc_provider:aud": "sts.amazonaws.com",
          "$oidc_provider:sub": "system:serviceaccount:$namespace:$service_account"
        }
      }
    }
  ]
}
EOF
```

### Step 5: Create IAM Role and Attach Policy
```bash
# Create role
aws iam create-role --role-name s3-access-default \
  --assume-role-policy-document file://trust-relationship.json \
  --description "s3 access role for pod SA s3-access/default"

# Attach policy
aws iam attach-role-policy --role-name s3-access-default \
  --policy-arn=arn:aws:iam::$account_id:policy/bespoke-pod-policy
```

### Step 6: Create and Annotate Service Account
```bash
# Create Service Account
cat >my-service-account.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-access
  namespace: default
EOF

kubectl apply -f my-service-account.yaml

# Annotate with IAM role
kubectl annotate serviceaccount -n $namespace $service_account \
  eks.amazonaws.com/role-arn=arn:aws:iam::$account_id:role/s3-access-default
```

### Step 7: Test Pod with IRSA
```bash
kubectl run --rm -ti cli --image=amazon/aws-cli \
  --overrides='{ "spec": { "serviceAccount": "s3-access" } }' \
  s3 ls
```

## Programmatic IRSA Creation with eksctl

### Simplified Command
```bash
eksctl create iamserviceaccount \
  --cluster=<clusterName> \
  --name=<serviceAccountName> \
  --namespace=<serviceAccountNamespace> \
  --attach-policy-arn=<policyARN>
```

### What eksctl Does Automatically
1. Determines EKS cluster OIDC provider
2. Creates role with trust policy
3. Attaches pre-created policy to IAM role
4. Creates Kubernetes Service Account with annotations

## Troubleshooting IRSA Issues

### Step 1: Identify IAM Permission Errors
Look for `AccessDenied` errors in AWS API operations:
```
An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied
```

### Step 2: Check Service Account
```bash
# Find which service account is being used
kubectl get po <pod-name> -o yaml | grep serviceAccountName

# Check service account annotations
kubectl describe sa <service-account-name>
```

### Step 3: Verify IAM Role and Policies
```bash
# List attached policies
aws iam list-attached-role-policies --role-name <role-name>

# Get policy details
aws iam get-policy --policy-arn <policy-arn>

# Get specific policy version
aws iam get-policy-version --policy-arn <policy-arn> --version-id v1
```

### Step 4: Validate Trust Relationship
```bash
# Check if role trusts correct OIDC provider
aws iam get-role --role-name <role-name>
```

### Step 5: Verify Pod Identity Webhook
```bash
kubectl get mutatingwebhookconfiguration pod-identity-webhook -o yaml
```

## Key Troubleshooting Areas
- Missing Service Account annotations
- Incorrect IAM policy permissions
- Wrong trust relationship configuration
- OIDC provider not configured
- Pod identity webhook issues

## Best Practices
- Use IMDSv2 with hop limit restrictions
- Implement least-privilege principle
- Give worker nodes minimal permissions only
- Use IRSA instead of inheriting worker node permissions
- Regularly audit and review IAM policies

## Summary
IRSA provides a secure way to assign specific AWS permissions to individual Pods without relying on worker node permissions. It uses Kubernetes Service Accounts mapped to IAM roles through OIDC identity providers, enabling fine-grained access control and following security best practices.

---

## AWS Fargate with EKS Notes

### Overview
AWS Fargate provides a serverless, container-native compute solution as an alternative to EC2-based worker nodes for EKS clusters. It's designed for workloads that are small, bursty, or batch-oriented.

### Prerequisites
- Network connectivity to EKS cluster API endpoint
- AWS CLI, Docker, and kubectl installed
- Basic understanding of AWS networking and EC2

## What is AWS Fargate?

### Design Tenets
1. **Security**: Maximum security isolation
2. **Reliability**: Scale to meet demand
3. **Cost-efficiency**: Optimized for specific workload patterns

### Architecture Comparison: Fargate vs EC2
- **Similar**: Both run on physical servers with VM OS and container runtime
- **Different**: Fargate is serverless - AWS manages the infrastructure layer
- **Optimization**: Fargate optimized for high utilization/density

### Suitable Workloads for Fargate
- **Bursty workloads**: Sporadic traffic patterns (high during day, low at night)
- **Test environments**: Occasionally used non-production environments
- **Batch jobs**: Periodic tasks like cron jobs or scheduled processes
- **Small workloads**: Applications with small memory/CPU footprints

### Firecracker Technology
- **MicroVM**: Small, secure, isolated environment using Virtual Machine Manager (VMM)
- **Multi-tenancy**: Single Fargate EC2 host supports multiple customer MicroVMs
- **Isolation**: Complete separation of kernels, CPU, memory, and ENIs between Pods
- **Single Pod per MicroVM**: Each MicroVM hosts exactly one Pod

## Fargate Pricing Model

### Pricing Structure
- **Duration-based**: Per-second granularity billing
- **Resource-based**: Based on vCPU, RAM, and disk allocation

### Cost Comparison Example (Frankfurt Region)
**EC2 Instance (2 CPU/4 GB, 30 GB disk, 100% uptime)**
- Cost: ~$21/month

**Fargate Pod (2 CPU/4 GB, 30 GB disk)**
- 5 hours/day: ~$10/month (less than half the EC2 cost)
- 10 hours/day: ~$35/month (significantly more expensive)

### Cost Considerations
- **Short-lived workloads**: Fargate is cost-effective
- **Long-running workloads**: EC2 typically more economical
- **Multiple Pods**: Each Fargate Pod incurs separate costs
- **EC2 utilization**: Most EC2 instances run at <30% utilization
- **Management overhead**: Factor in operational costs of managing EC2 vs serverless

## Creating AWS Fargate Profile in EKS

### How Fargate Profile Works
The profile determines which Pods should run on Fargate without requiring changes to existing Kubernetes manifests.

### Fargate Deployment Workflow
1. **Pod Create Request**: API server receives Pod creation request
2. **Webhook Processing**: Custom AWS webhook checks Fargate profile for namespace/label match
3. **Scheduler Assignment**: If matched, webhook changes scheduler to Fargate Scheduler
4. **Intent Storage**: API server writes to etcd with appropriate scheduler
5. **Fargate Orchestration**: Fargate Scheduler provisions MicroVM and attaches to customer VPC

### Creating Fargate Profile with eksctl

#### Step 1: Create Namespace
```bash
kubectl create namespace fargate-workload
```

#### Step 2: Create Fargate Profile
```bash
eksctl create fargateprofile \
  --cluster myipv4cluster \
  --name fargate \
  --namespace fargate-workload
```

#### Step 3: Verify Profile
```bash
eksctl get fargateprofile --cluster myipv4cluster -o yaml
```

**Example Output:**
```yaml
- name: fargate
  podExecutionRoleARN: arn:aws:iam::112233:role/eksctl-myipv4cluster-farga-FargatePodExecutionRole-1CD7AYHOTBDYO
  selectors:
  - namespace: fargate-workload
  status: ACTIVE
  subnets:
  - subnet-private1
  - subnet-private2
  - subnet-private3
```

**Note**: Maximum of 10 profiles per cluster

## Deploying Pods to Fargate

### Basic Deployment
Simply change the namespace in existing manifests to match Fargate profile:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: simple-web
  namespace: fargate-workload
spec:
  replicas: 1
  selector:
    matchLabels:
      app: simple-nginx-app
  template:
    metadata:
      labels:
        app: simple-nginx-app
    spec:
      containers:
        - name: nginx
          image: nginx
```

### Verification Commands
```bash
# Check Pod deployment
kubectl get po -n fargate-workload -o wide

# Verify scheduler assignment
kubectl get pods -o yaml -n fargate-workload simple-web-12 | grep schedulerName

# Check Fargate nodes
kubectl get node
```

### Resource Configuration

#### Default Resources
- **Default**: 0.25 vCPU and 0.5 GB memory (smallest available)
- **Storage**: 20 GB ephemeral storage per Pod

#### Custom Resource Limits
```yaml
containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "2Gi"
        cpu: "2000m"
```

**Note**: Fargate rounds up to nearest supported CPU/memory combination and adds 246 MB RAM overhead for Kubernetes components.

### Pod Disruption Budgets (PDBs)
Protect against Pod evictions during AWS maintenance:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: fg-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: simple-nginx-app
```

Verify PDB:
```bash
kubectl get poddisruptionbudgets -n fargate-workload
```

## Troubleshooting Common Fargate Issues

### Capacity Issues
**Symptom**: Pod status stuck in PENDING
**Causes**:
- Insufficient platform resources
- Unsupported CPU/RAM combination

**Solutions**:
- Wait 15-20 minutes and retry
- Adjust Pod spec to different CPU/RAM combination

### Node Not Ready Status
**Symptom**: Fargate nodes show "Not Ready"
**Cause**: Execution role not configured in aws-auth ConfigMap

**Solution**: Add execution role to aws-auth ConfigMap:
```yaml
mapRoles: |
  - groups:
    - system:bootstrappers
    - system:nodes
    - system:node-proxier
    rolearn: <Pod_execution_role_ARN>
    username: system:node:{{SessionName}}
```

### CoreDNS Issues
**Symptom**: CoreDNS Pods in PENDING state
**Cause**: VPC DNS not configured properly

**Solution**: Ensure VPC settings:
- `enableDNSHostnames`: True
- `enableDNSSupport`: True

### Profile Matching Rules
- **All conditions**: Both namespace AND labels must match if both are specified in profile
- **Multiple profiles**: Random profile selection if Pod matches multiple profiles
- **Exact matching**: Profile selectors must exactly match Pod specifications

## Fargate Limitations and Considerations

### Network Limitations
- **Private subnets only**: Public subnets not supported
- **VPC connectivity**: Pods only accessible from within VPC
- **Load balancers**: Use ALB/NLB for external access

### Resource Constraints
- **Predefined sizes**: Limited to AWS-defined CPU/memory combinations
- **Single Pod per MicroVM**: No Pod co-location
- **Ephemeral storage**: 20 GB limit, deleted with Pod

### Operational Considerations
- **Patching**: AWS may patch and evict Pods during maintenance
- **Cold starts**: Initial Pod startup may be slower than EC2
- **Networking**: Uses same security groups as EC2 worker nodes

## Best Practices

### When to Use Fargate
- ✅ **Ideal for**:
  - Batch jobs and scheduled tasks
  - Development/testing environments
  - Workloads with variable traffic patterns
  - Security-sensitive applications requiring isolation

- ❌ **Avoid for**:
  - Long-running, steady-state workloads
  - High-throughput applications
  - Cost-sensitive applications running 24/7

### Configuration Best Practices
- Use Pod Disruption Budgets for critical workloads
- Set appropriate resource limits to control costs
- Use private subnets with proper VPC configuration
- Monitor costs closely for usage patterns

## Summary
AWS Fargate provides a serverless container platform that eliminates the need to manage EC2 worker nodes. It uses Firecracker MicroVMs for complete isolation and is ideal for bursty, batch, or security-sensitive workloads. While more expensive for long-running applications, it can be cost-effective for intermittent workloads and eliminates operational overhead.

## Further Reading
- [Understanding the Firecracker design](https://github.com/firecracker-microvm/firecracker/blob/main/docs/design.md)
- [Understanding how Fargate is priced](https://aws.amazon.com/fargate/pricing/)
- [Understanding Fargate Pod configurations](https://docs.aws.amazon.com/eks/latest/userguide/fargate-pod-configuration.html)