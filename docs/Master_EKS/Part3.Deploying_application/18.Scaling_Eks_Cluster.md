# Scaling Your EKS Cluster - Notes

## Overview

Capacity planning on EKS can be challenging due to unpredictable application loads. This chapter covers strategies and tools for optimizing EKS clusters for both load handling and cost efficiency.

## Key Topics Covered

- Understanding scaling in EKS
- Scaling EC2 ASGs with Cluster Autoscaler
- Scaling worker nodes with Karpenter
- Scaling applications with Horizontal Pod Autoscaler (HPA)
- Scaling applications with custom metrics
- Scaling with KEDA

## Prerequisites

- Familiarity with YAML, AWS IAM, and EKS architecture
- Network connectivity to EKS cluster API endpoint
- AWS CLI, Docker, and kubectl installed with admin access

## Understanding Scaling in EKS

### Scaling Dimensions

1. **Vertical Scaling**: Increasing the size of a system/instance
2. **Horizontal Scaling**: Increasing the number of systems/instances

### EKS Scaling Model

- **Control Plane**: Managed by AWS (scaling and resilience handled automatically)
- **Data Plane**: Worker nodes - our responsibility
- **Application Resources**: Pods/containers - our responsibility

### EKS Scaling Technology Stack

#### 1. AWS Technology Layer
- EC2 Autoscaling Groups (ASGs)
- AWS APIs for ASG interaction
- Fargate service for serverless scaling

#### 2. Kubernetes Objects Layer
- Deployments
- ReplicaSets
- StatefulSets
- DaemonSets

#### 3. Kubernetes Scheduler and Controllers
- Cluster Autoscaler (CA)
- Karpenter
- Horizontal Pod Autoscaler (HPA)
- KEDA

## Cluster Autoscaler (CA)

### Overview
- Core part of K8s ecosystem
- Scales worker nodes based on pod scheduling requirements

### How It Works
1. Monitors pods in `Pending` state due to insufficient resources
2. Calls EC2 ASG API to increase desired capacity
3. New nodes register with cluster
4. Scheduler places pending pods on new nodes

### Installation Requirements

#### Subnet Tagging
```bash
# Required tags for subnets
k8s.io/cluster-autoscaler/enabled=true
k8s.io/cluster-autoscaler/<cluster-name>=owned
```

#### IAM Policy
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "autoscaling:ResourceTag/k8s.io/cluster-autoscaler/enabled": "true",
                    "aws:ResourceTag/k8s.io/cluster-autoscaler/<cluster-name>": "owned"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeAutoScalingGroups",
                "autoscaling:DescribeScalingActivities",
                "ec2:DescribeLaunchTemplateVersions",
                "autoscaling:DescribeTags",
                "autoscaling:DescribeLaunchConfigurations",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        }
    ]
}
```

### Installation Steps

1. **Create IAM Policy and Service Account**
```bash
aws iam create-policy --policy-name AmazonEKSClusterAutoscalerPolicy \
--policy-document file://autoscaler_policy.json

eksctl create iamserviceaccount \
--cluster=<cluster-name> \
--namespace=kube-system \
--name=cluster-autoscaler \
--attach-policy-arn=arn:aws:iam::<account>:policy/AmazonEKSClusterAutoscalerPolicy \
--override-existing-serviceaccounts \
--approve
```

2. **Download and Modify Manifest**
- Download from: `https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml`
- Modify autodiscovery tag (line 165)
- Add command switches for better balancing
- Update container image to match K8s version

3. **Deploy**
```bash
kubectl create -f cluster-autoscaler-autodiscover.yaml
kubectl patch deployment cluster-autoscaler -n kube-system -p '{"spec":{"template":{"metadata":{"annotations":{"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"}}}}}'
```

### Testing
- Set ASG maximum capacity higher than current
- Deploy high-replica deployment to trigger scaling
- Monitor with: `kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler`

## Karpenter

### Overview
- Open source autoscaling solution for Kubernetes
- Builds and removes capacity without need for node groups/ASGs
- Faster scaling than Cluster Autoscaler
- Supports multiple instance types dynamically

### How It Works
1. Monitors pods in `Pending` state
2. Reviews resource requests, node selectors, affinities, tolerations
3. Provisions nodes that meet pod requirements
4. Scheduler places pods on new nodes

### Installation Requirements

#### Environment Variables
```bash
export CLUSTER_NAME=<your-cluster-name>
export AWS_PARTITION="aws"
export AWS_REGION="$(aws configure list | grep region | tr -s " " | cut -d" " -f3)"
export OIDC_ENDPOINT="$(aws eks describe-cluster --name ${CLUSTER_NAME} --query "cluster.identity.oidc.issuer" --output text)"
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
```

#### IAM Roles and Policies

1. **Instance Node Role**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Required managed policies:
- `AmazonEKSWorkerNodePolicy`
- `AmazonEKS_CNI_Policy`
- `AmazonEC2ContainerRegistryReadOnly`
- `AmazonSSMManagedInstanceCore`

2. **Controller Role**
- Trust policy for OIDC
- Custom policy with EC2, SSM, IAM permissions

#### Tagging Requirements
- Subnets: `karpenter.sh/discovery=<cluster-name>`
- Security Groups: `karpenter.sh/discovery=<cluster-name>`

### Configuration

#### Provisioner Example
```yaml
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  labels:
    type: karpenter
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["on-demand"]
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values: ["c5.large", "m5.large", "m5.xlarge"]
  limits:
    resources:
      cpu: 1000
      memory: 1000Gi
  providerRef:
    name: default
  ttlSecondsAfterEmpty: 30
```

#### AWSNodeTemplate Example
```yaml
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeTemplate
metadata:
  name: default
spec:
  subnetSelector:
    karpenter.sh/discovery: <cluster-name>
  securityGroupSelector:
    karpenter.sh/discovery: <cluster-name>
```

### Advantages over Cluster Autoscaler
- Much faster scaling (seconds vs minutes)
- No need for pre-defined node groups
- Dynamic instance type selection
- Better resource optimization

## Horizontal Pod Autoscaler (HPA)

### Overview
- Kubernetes component for automatic pod scaling
- Scales pods based on metrics (CPU, memory, custom metrics)
- Requires Metrics Server

### Prerequisites
```bash
# Check if Metrics Server is installed
kubectl -n kube-system get deployment/metrics-server

# Install if needed
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### Basic HPA Usage

#### Example Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
```

#### Create HPA
```bash
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
```

### Load Testing
```bash
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"
```

## Custom Metrics with HPA

### Architecture Components
1. **Application**: Instrumented to produce metrics
2. **Prometheus Server**: Scrapes custom metrics from pods
3. **Prometheus Adapter**: Exposes metrics via `custom.metrics.k8s.io` endpoint
4. **HPA**: Reads custom metrics and scales deployment

### Installation Steps

#### 1. Install Prometheus
```bash
kubectl create namespace prometheus
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade -i prometheus prometheus-community/prometheus \
--namespace prometheus \
--set alertmanager.persistentVolume.storageClass="gp2",server.persistentVolume.storageClass="gp2"
```

#### 2. Deploy Application (podinfo example)
```bash
kubectl create ns podinfo
kubectl apply -k github.com/stefanprodan/podinfo/kustomize -n podinfo
```

#### 3. Install Prometheus Adapter
```bash
helm install prometheus-adapter prometheus-community/prometheus-adapter \
--set prometheus.url="http://prometheus-server.prometheus.svc",prometheus.port="80" \
--set image.tag="v0.10.0" \
--set rbac.create="true" \
--namespace prometheus
```

#### 4. Configure Custom Metrics
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter
  namespace: prometheus
data:
  config.yaml: |
    rules:
    - seriesQuery: 'http_requests_total'
      resources:
        overrides:
          namespace:
            resource: "namespace"
          pod:
            resource: "pod"
      name:
        matches: "^(.*)_total"
        as: "${1}_per_second"
      metricsQuery: 'rate(http_requests_total{namespace="podinfo",app="podinfo"}[2m])'
```

#### 5. Create Custom HPA
```yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: podinfo
  namespace: podinfo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Pods
      pods:
        metricName: http_requests_per_second
        targetAverageValue: 10
```

## KEDA (Kubernetes Event-Driven Autoscaling)

### Overview
- Open source framework for event-driven scaling
- Supports internal and external event sources
- More flexible than standard HPA
- Better for fluctuating, non-deterministic demand

### Components
1. **Agent**: Scales deployment up/down based on events
2. **Metrics Server**: Exposes metrics from applications/external sources
3. **ScaledObject**: Maps external sources to K8s deployments
4. **Event Sources**: Internal and external triggers

### Installation
```bash
helm repo add kedacore https://kedacore.github.io/charts
helm repo update
kubectl create namespace keda
helm install keda kedacore/keda --namespace keda --version 2.9.4
```

### ScaledObject Configuration
```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-scaledobject
  namespace: podinfo
spec:
  scaleTargetRef:
    name: podinfo
  minReplicaCount: 2
  maxReplicaCount: 10
  pollingInterval: 10
  cooldownPeriod: 30
  fallback:
    failureThreshold: 3
    replicas: 2
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server.prometheus.svc.cluster.local:80
      metricName: http_requests_total
      threshold: '10'
      query: sum(rate(http_requests_total{namespace="podinfo",app="podinfo"}[2m]))
```

### Advantages
- Dynamic replica adjustment
- Scales to zero (if minReplicaCount=0)
- Multiple event source support
- Faster response to demand changes

## Best Practices and Considerations

### Cluster Autoscaler vs Karpenter
| Feature | Cluster Autoscaler | Karpenter |
|---------|-------------------|-----------|
| Speed | Slower (10+ minutes) | Faster (seconds) |
| Instance Types | Fixed per node group | Dynamic selection |
| Node Groups | Required | Not needed |
| Complexity | Higher setup | Simpler configuration |
| Maturity | More mature | Newer, actively developed |

### HPA vs KEDA
| Feature | HPA | KEDA |
|---------|-----|------|
| Metrics | CPU, Memory, Custom | Any external source |
| Scale to Zero | No | Yes |
| Event Sources | Limited | Extensive |
| Configuration | Simpler | More flexible |
| Production Use | Well established | Growing adoption |

### Monitoring and Troubleshooting

#### Cluster Autoscaler Logs
```bash
kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler
```

#### HPA Status
```bash
kubectl get hpa --watch
kubectl describe hpa <hpa-name>
```

#### Custom Metrics Verification
```bash
kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/<namespace>/pods/*/http_requests_per_second"
```

## Common Issues and Solutions

### Cluster Autoscaler
- **Issue**: Nodes not scaling up
  - **Solution**: Check ASG max capacity, node tagging, IAM permissions
- **Issue**: Slow scaling
  - **Solution**: Consider switching to Karpenter for faster scaling

### Karpenter
- **Issue**: Nodes not provisioning
  - **Solution**: Verify subnet/security group tagging, IAM roles
- **Issue**: Wrong instance types selected
  - **Solution**: Review Provisioner requirements and limits

### HPA
- **Issue**: Pods not scaling
  - **Solution**: Check Metrics Server, resource requests, HPA configuration
- **Issue**: Custom metrics not available
  - **Solution**: Verify Prometheus scraping, adapter configuration

### KEDA
- **Issue**: ScaledObject not active
  - **Solution**: Check external data source connectivity, query syntax
- **Issue**: Erratic scaling behavior
  - **Solution**: Adjust pollingInterval and cooldownPeriod

## Further Reading

- [EKS Observability Tools](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html)
- [Podinfo Documentation](https://github.com/stefanprodan/podinfo)
- [KEDA Documentation](https://keda.sh/docs/2.10/concepts/)
- [Karpenter Official Documentation](https://karpenter.sh)
- [Prometheus Adapter with Amazon Managed Prometheus](https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/)