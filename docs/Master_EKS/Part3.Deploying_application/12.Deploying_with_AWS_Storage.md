# Deploying Pods with Amazon Storage

## Overview

A guiding principle in most distributed or cloud-native applications is to limit the state where you can, and so far we have deployed Pods that are stateless, which means that when they are destroyed and recreated, any data they had is lost. In some cases, you might want to share data between containers in the Pod, maintain the state or content between reboots/re-deployments/crashes, or share data between Pods, in which case you need some sort of persistent storage.

This chapter looks at how you can use Elastic Block Storage (EBS) and Elastic File System (EFS) to persist the container state or content across multiple containers, Pods, or deployments.

### Key Topics Covered

- Understanding Kubernetes volumes, the Container Storage Interface (CSI) driver, and storage on AWS
- Installing and configuring the AWS CSI drivers in your cluster
- Using EBS volumes with your application
- Using EFS volumes with your application

## Technical Requirements

You should be familiar with YAML, basic networking, and Elastic Kubernetes Service (EKS) architecture. Before getting started with this chapter, please ensure that you have the following:

- Network connectivity to your EKS cluster API endpoint
- The AWS command-line interface (CLI), Docker, and kubectl binary installed on your workstation
- A basic understanding of block and file storage systems

## Understanding Kubernetes Volumes, CSI Driver, and Storage on AWS

### Basic Storage Concepts

The basic storage object within Kubernetes is a **volume**, which represents a directory (with or without data) that can be accessed by containers in a Pod. 

#### Volume Types

**Ephemeral Volumes**
- Persist over container restarts
- Aligned to the lifetime of the Pod
- Destroyed by the Kubernetes scheduler when the Pod is destroyed

**Persistent Volumes**
- Not destroyed by Kubernetes
- Exist separately from the Pod or Pods that use them

### Ephemeral Volume Examples

#### emptyDir Volume Type

The simplest example of an ephemeral volume is an **emptyDir** volume type. This mounts host storage inside the containers using the `mountPath` key. As both containers use the same volume, they see the same data despite the fact it's mounted onto different mount points. When a Pod dies, crashes, or is removed from a node, the data in the emptyDir volume is deleted and lost:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: empty-dir-example
spec:
  volumes:
  - name: shared-data
    emptyDir: {}
  containers:
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  - name: debian-container
    image: debian
    volumeMounts:
    - name: shared-data
      mountPath: /pod-data
    command: ["/bin/sh"]
    args: ["-c", "echo Hello from Debian > /pod-data/index.html"]
```

!!! note "Important Note"
    The Pod will crash after it has run the echo command, but this is expected, so rest assured.

#### hostPath Volume Type

You can also create a persistent host volume using the **hostPath** type. In this example, a volume is created and mapped to the host `/data` directory, which in turn, is mounted in the nginx container using the `mountPath` key. The main difference between this configuration and the previous emptyDir volume type is that any data stored on the volume will be persisted in the `/data` directory on the host even if the Pod is deleted:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: host-path-example
spec:
  containers:
  - image: nginx
    name: test
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: shared-data
  volumes:
  - name: shared-data
    hostPath:
      path: /data
```

The main challenge to these types of volumes is they are host-specific and so if there are any issues with the hosts, or if the Pod is scheduled on another host, the data is lost or not reachable.

### Container Storage Interface (CSI)

Kubernetes started with more volume types, such as `awsElasticBlockStore`, which uses an external/non-host AWS resource and removes some of these constraints. The plugins that supported these external volume types were known as **in-tree**, as they were developed by the Kubernetes community. However, the effort required to support changes in the volume configuration and different volume types became too much, so the CSI was made generally available in Kubernetes 1.13.

The CSI specification acts as a way to expose block and file-based storage consistently, irrespective of the storage type or vendor. The CSI allows AWS (and other vendors) to develop and support storage drivers for its storage services, namely EBS and EFS.

## AWS Storage Systems Deep Dive

### Elastic Block Store (EBS)

EBS is block-based storage that is typically attached to a single Elastic Compute Cloud (EC2) instance or Pod in a single availability zone (AZ). 

#### EBS Characteristics

- **Performance Types**: 
  - General-purpose: gp3 and gp2
  - High-performance: io1 and io2
  - Storage Types: SSD and HDD (magnetic)
- **Billing**: Gigabyte-month (GB-month) - a measure of how many gigabytes of EBS storage are provisioned in your account and the length of time it is used for
- **Multi-Attach**: While EBS now supports attaching up to 16 nitro-based EC2 instances in the same AZ to a single EBS volume, this is a relatively new configuration option

### Elastic File System (EFS)

EFS is file-based storage based on NFS (NFSv4.1 and NFSv4.0), which allows multiple EC2 instances or Pods to access shared storage across multiple AZs.

#### EFS Characteristics

- **Availability**: Regional (multi-AZ) or span a single AZ
- **Storage Classes**: Standard and infrequently accessed data patterns
- **Billing**: Based on the amount of storage used per month, measured in GB-months, as well as the storage class used and the duration of storage usage

### Choosing Between EBS and EFS

The criteria used to choose between EBS and EFS vary but, generally:

- **EFS**: Good candidate if you want a shared storage solution that can be used across multiple AZs
- **EBS**: Normally used to provide persistent volumes within a single AZ with high throughput

## Installing and Configuring AWS CSI Drivers

We will install both the EBS and EFS drivers. You will need a similar process for both:

1. Create an Identity and Access Management (IAM) policy that will allow the plugin to perform AWS API calls for either EBS or EFS
2. Create and map an IAM role to an EKS service account
3. Deploy the plugin and configure it to use the service account created in step 2

### Installing and Configuring the EBS CSI Driver

The driver can be found at [https://github.com/kubernetes-sigs/aws-ebs-csi-driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver).

#### Step 1: Create IAM Policy

You can create the IAM policy from scratch or you can use the `AmazonEBSCSIDriverPolicy` AWS-managed policy.

#### Step 2: Create IAM Role

We can now create the role. We will use the `--role-only` command-line switch, so we don't create the EKS service account. Using the following eksctl command, adjust the command line parameters as necessary:

```bash
eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster myipv4cluster \
  --override-existing-serviceaccounts \
  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  --approve \
  --role-name AmazonEKS_EBS_CSI_DriverRole \
  --role-only
```

!!! note "Important Note"
    In the preceding example, we used the cluster we created in Chapter 9. If you use a different cluster, you will need to change the `--cluster` parameter to reflect your cluster name.

#### Step 3: Create Add-on

You can create an add-on for the EBS CSI controller using the following eksctl command, which will deploy the CSI Pods and also the service accounts needed to access the AWS API using the role created in step 2:

```bash
eksctl create addon \
  --name aws-ebs-csi-driver \
  --cluster myipv4cluster \
  --service-account-role-arn arn:aws:iam::11223344:role/AmazonEKS_EBS_CSI_DriverRole \
  --force
```

Expected output:
```
2022-09-22 19:59:19 []  Kubernetes version "1.20" in use by cluster "myipv4cluster"
………
2022-09-22 20:00:28 []  addon "aws-ebs-csi-driver" active
```

#### Validation

You can validate whether the controller and DaemonSets are deployed using the following commands:

```bash
kubectl get pods -n kube-system | grep ebs
```

Expected output:
```
ebs-csi-controller-2233-p75xg   6/6     Running   1
ebs-csi-controller-3444-rb9zg   6/6     Running   0
ebs-csi-node-94pgc              3/3     Running   0
ebs-csi-node-mwdqc              3/3     Running   0
ebs-csi-node-t9h77              3/3     Running   0
```

Check logs:
```bash
kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner
```

Expected log entry:
```
I0922 19:59:53.169651       1 leaderelection.go:258] successfully acquired lease kube-system/ebs-csi-aws-com
```

### Installing and Configuring the EFS CSI Driver

The driver can be found at [https://github.com/kubernetes-sigs/aws-efs-csi-driver](https://github.com/kubernetes-sigs/aws-efs-csi-driver).

#### Step 1: Create IAM Policy

You can create the IAM policy from scratch or you can use the example policy found here: [https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json](https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json). 

The following commands can be used to download and create the IAM policy:

```bash
curl -o iam-policy-example.json https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json
aws iam create-policy --policy-name AmazonEKS_EFS_CSI_Driver_Policy --policy-document file://iam-policy-example.json
```

#### Step 2: Create IAM Role and Service Account

We can now create the role and associated EKS service account using the following eksctl command and adjust the command line parameters as necessary (you will need to specify the Amazon Resource Name (ARN) of the policy created in step 1, as well as the cluster name and Region). The most important aspect to verify is that the new service account has an annotation for the new IAM role:

```bash
eksctl create iamserviceaccount \
  --cluster myipv4cluster \
  --namespace kube-system \
  --name efs-csi-controller-sa \
  --attach-policy-arn arn:aws:iam::11223344:policy/AmazonEKS_EFS_CSI_Driver_Policy \
  --approve \
  --region eu-central-1
```

Expected output:
```
2022-09-22 20:32:29 []  3 existing iamserviceaccount(s) (kube-system/ebs-csi-controller-sa,kube-system/eni-allocator,kube-system/multus) will be exclude
………
2022-09-22 20:32:59 []  created serviceaccount "kube-system/efs-csi-controller-sa"
```

Verify the service account:
```bash
kubectl describe sa efs-csi-controller-sa -n kube-system
```

Expected output:
```
Name:                efs-csi-controller-sa
……
Annotations:         eks.amazonaws.com/role-arn: arn:aws:iam::076637564853:role/eksctl-myipv4cluster-addon-iamserviceaccount-Role1-P08589EN3NY7
…..
```

#### Step 3: Install Using Helm

We will use Helm to install this EFS CSI driver as, unlike the EBS driver, at the time of writing, the EFS driver is not supported as an add-on. The following command will add the EFS repository to Helm and deploy the Helm chart, re-using the EKS service account that was created in step 2:

!!! note "Important Note"
    `Image.repository` is Region-specific, and the relevant repositories can be found at [https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html](https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html).

```bash
helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/
helm repo update
helm search repo aws-efs-csi-driver
```

Expected output:
```
NAME                                    CHART VERSION   APP VERSION     DESCRIPTION
aws-efs-csi-driver/aws-efs-csi-driver   2.2.7           1.4.0           A Helm chart for AWS EFS CSI Driver
```

Install the driver:
```bash
helm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
  --namespace kube-system \
  --set image.repository=602401143452.dkr.ecr.eu-central-1.amazonaws.com/eks/aws-efs-csi-driver \
  --set controller.serviceAccount.create=false \
  --set controller.serviceAccount.name=efs-csi-controller-sa
```

#### Validation

Check pods:
```bash
kubectl get pod -n kube-system -l "app.kubernetes.io/name=aws-efs-csi-driver,app.kubernetes.io/instance=aws-efs-csi-driver"
```

Expected output:
```
NAME                                  READY   STATUS    RESTARTS   AGE
efs-csi-controller-122-hrzfg          3/3     Running   0
efs-csi-controller-1234-q8wpt         3/3     Running   0
efs-csi-node-2g46k                    3/3     Running   0
efs-csi-node-59rsx                    3/3     Running   0
efs-csi-node-ncsk8                    3/3     Running   0
```

Check logs:
```bash
kubectl logs deployment/efs-csi-controller -n kube-system -c csi-provisioner
```

Expected log entry:
```
I0922 20:51:53.306805       1 leaderelection.go:253] successfully acquired lease kube-system/efs-csi-aws-com
```

!!! note "Important Note"
    The EFS plugin will require a pre-configured EFS cluster to be available; we will discuss how this can be created in the Using EFS volumes with your application section.

## Using EBS Volumes with Your Application

Kubernetes has three main kinds that are used for persistent storage:

- **PersistentVolume (PV)**: Represents the actual storage in the attached storage system, in our case, an EBS volume
- **StorageClass (SC)**: Defines the characteristics of the storage
- **PersistentVolumeClaim (PVC)**: Represents a request for storage that is fulfilled by a PV based on an SC

The reason a PVC exists is that different Pods may require different types of storage, for example, storage shared between many Pods or dedicated to just one. The PVC provides an abstraction between what a developer or DevOps engineer needs for their application and the type of storage provided by the cluster administrator.

### EBS Volume Architecture

The following diagram illustrates the relationship between an EBS volume, PV, PVC, and a Pod:

```
┌─────────────┐    ┌──────────────┐    ┌─────────────┐    ┌─────────────┐
│     Pod     │────│     PVC      │────│      PV     │────│EBS Volume   │
│             │    │              │    │             │    │(AWS)        │
└─────────────┘    └──────────────┘    └─────────────┘    └─────────────┘
```

### Important EBS Considerations

- An EBS volume is specific to a Region and an AZ
- You can't move EBS volumes between AZs
- To move data, you need to create a snapshot and then create a new volume in the new AZ
- A PV (EBS volume) can be created statically by an AWS administrator or dynamically as you consume a PVC
- It can only be consumed by worker nodes in the same AZ as the volume itself

### Creating EBS Storage Resources

#### Step 1: Create StorageClass

We will focus on the dynamic creation of the volumes, as this is the simplest method to implement. The latest EBS CSI driver automatically creates a gp2 SC and this can be viewed by using the following commands:

```bash
kubectl get storageclass
```

Expected output:
```
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  50d
```

We want to use gp3, which is a more cost-effective form of storage and performant type on AWS, so let's create a new SC using the following manifest:

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gp3
provisioner: ebs.csi.aws.com # Amazon EBS CSI driver
parameters:
  type: gp3
  encrypted: 'true'
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
```

Deploy using:
```bash
kubectl create -f SC-config.yaml
```

#### Step 2: Create PersistentVolumeClaim

We can now create a PVC that will leverage the new SC. As we are using dynamic provisioning, we don't have to create a PV, as this will be created once we deploy a Pod that references the new PVC.

The following manifest will create the PVC:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp3
  resources:
    requests:
      storage: 4Gi
```

Deploy using:
```bash
kubectl create -f VC-config.yaml
```

The manifest contains:
- **SC**: `gp3` (the StorageClass to use)
- **Size**: `4Gi` (size of the volume to create)
- **Encryption**: Not specified in PVC, but inherited from SC (volume will be encrypted)

### Access Modes

`accessModes` defines how the volume can be attached. EBS will only support `ReadWriteOnce`:

- **ReadWriteOnce (RWO)**: This volume can be mounted as read-write by a single node
- **ReadOnlyMany (ROX)**: This volume can be mounted read-only by many nodes
- **ReadWriteMany (RWX)**: This volume can be mounted as read-write by many nodes
- **ReadWriteOncePod (RWOP)**: This volume can be mounted as read-write by a single Pod

#### Verify PVC Status

The following commands show the PVC being created in a pending state (as no Pod has made a claim against it), and no associated EBS volume (PV) has been created, as the PVC is still in a pending state:

```bash
kubectl create -f ebs-pvc.yaml
kubectl get pvc
kubectl get pv
```

Expected output:
```
persistentvolumeclaim/ebs-claim created

NAME        STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ebs-claim   Pending                                       gp3            10s

No resources found
```

#### Step 3: Deploy Pod with EBS Volume

We can now deploy a Pod that uses this PVC, which, in turn, will, using the EBS CSI driver, create a new EBS volume (dynamic provisioning) and attach it to the Pod as specified by `mountPath` in the Pod specification.

!!! note "Important Note"
    It's worth noting that the Pod deployment time will take longer as the EBS volume needs to be created first. If a quicker startup time is needed, then static provisioning can be used and the PV can be created before the Pod.

The following manifest will create the Pod and references the PVC created previously:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: debian
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
```

Deploy using:
```bash
kubectl create -f ebs-pod.yaml
```

#### Verify Deployment

We use the following commands to verify the successful deployment of the Pod and the creation of the EBS volume. Once the Pod is created, we can see the PVC is now in a Bound state and a new PV is created also in a Bound state:

```bash
kubectl create -f ebs-pod.yaml
kubectl get pvc
kubectl get pv
```

Expected output:
```
pod/app created

NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ebs-claim   Bound    pvc-18661cce-cda5-4779-86b4-21cb76a5ecc0   4Gi        RWO            gp3            30m

NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   AGE
pvc-18661cce-cda5-4779-86b4-21cb76a5ecc0   4Gi        RWO            Delete           Bound    default/ebs-claim   gp3            17s
```

#### View Volume Details

If we look in detail at the PV, we can see the ID of the volume created in AWS by looking at the `VolumeHandle` key:

```bash
kubectl describe pv pvc-18661cce-cda5-4779-86b4-21cb76a5ecc0
```

Expected output:
```
Name:              pvc-18661cce-cda5-4779-86b4-21cb76a5ecc0
Labels:            <none>
…
StorageClass:      gp3
Status:            Bound
Claim:             default/ebs-claim
Reclaim Policy:    Delete
Access Modes:      RWO
VolumeMode:        Filesystem
Capacity:          4Gi
Node Affinity:
  Required Terms:
    Term 0: topology.ebs.csi.aws.com/zone in [eu-central-1b]
Message:
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            ebs.csi.aws.com
    FSType:            ext4
    VolumeHandle:      vol-00f7c5d865ef5c14f
    ReadOnly:          false
    …
```

### Reclaim Policy

When the PVC is removed, the reclaim policy (defaulting to what is defined in the SC for dynamic provisioning) dictates what happens. In the previous example, `Reclaim Policy` is `Delete`, which means the Kubernetes resources (the PV and PVC) will be deleted, along with the associated EBS volume. If you want to preserve the EBS volume, then the `Retain` value should be set in the SC.

### AWS Console Verification

Now, go into the AWS console and search for the volume ID. The example shown next illustrates the volume, the provisioned size, and the type, along with the Key Management Service (KMS) details and throughput:

![AWS Console EBS Volumes](images/aws-console-ebs-volumes.png)

## Using EFS Volumes with Your Application

EFS is a shared storage platform unlike EBS, so while at the Kubernetes level, you have the same objects, SC, PV, and PVCs, the way you access the storage and how the storage is created are quite different.

### EFS Volume Architecture

The following diagram illustrates the relationship between an EFS instance/volume and the Kubernetes PV, PVC, and Pod:

```
┌─────────────┐    ┌──────────────┐    ┌─────────────┐    ┌─────────────┐
│   Pod 1     │────│     PVC      │────│      PV     │────│EFS Instance │
├─────────────┤    │              │    │             │    │(AWS)        │
│   Pod 2     │────│              │    │             │    │             │
├─────────────┤    │              │    │             │    │             │
│   Pod 3     │────│              │    │             │    │             │
└─────────────┘    └──────────────┘    └─────────────┘    └─────────────┘
```

Although we have installed the CSI driver, we can't provision volumes without an EFS instance and mount targets in the required subnets.

### Creating the EFS Instance and Mount Targets

You can do this in a variety of ways, but we will use the AWS CLI.

#### Step 1: Create EFS Filesystem

Let's start by creating the EFS filesystem and retrieving the filesystem ID. The following command will create the EFS instance and filter the response to only return FileSystemId. Please adjust the `--region` parameter to account for the Region you're using:

```bash
aws efs create-file-system \
  --region eu-central-1 \
  --performance-mode generalPurpose \
  --query 'FileSystemId' \
  --output text
```

Expected output:
```
fs-078166286587fc22
```

#### Step 2: Identify Subnets for Mount Targets

The next step is to identify the subnets we want to use for our mount targets. Ideally, we place the mount targets in the same subnets as the worker nodes. The following commands will list all the subnets for a given virtual private cloud (VPC) (you will need to supply the correct VPC-ID) and then list which subnets and security groups are being used by your managed node group:

```bash
aws ec2 describe-subnets \
  --filters "Name=vpc-id,Values=vpc-123" \
  --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \
  --output table
```

Expected output:
```
+------------------+--------------------+-----------------+
| AvailabilityZone |     CidrBlock      |   SubnetId      |
+------------------+--------------------+-----------------+
|  eu-central-1a   |  192.168.96.0/19   |  subnet-1       |
|  eu-central-1b   |  192.168.32.0/19   |  subnet-2       |
|  eu-central-1a   |  192.168.0.0/19    |  subnet-3       |
|  eu-central-1c   |  192.168.160.0/19  |  subnet-4       |
|  eu-central-1c   |  192.168.64.0/19   |  subnet-5       |
+------------------+--------------------+-----------------+
```

List worker node subnets:
```bash
aws ec2 describe-instances \
  --filters "Name=tag:aws:eks:cluster-name,Values=myipv4cluster" \
  --query "Reservations[*].Instances[*].{Instance:InstanceId,Subnet:SubnetId,PrivateIP:PrivateIpAddress}" \
  --output table
```

Expected output:
```
+---------------------+------------------+----------------+
|      Instance       |    PrivateIP     |    Subnet      |
+---------------------+------------------+----------------+
|  i-01437f1b219217d8a|  192.168.12.212  |  subnet-3      |
|  i-0f74d4d5b7e5dc146|  192.168.70.114  |  subnet-5      |
|  i-04c48cc4d2ac11ca6|  192.168.63.61   |  subnet-2      |
+---------------------+------------------+----------------+
```

We can see from the previous output that we should create mount points in subnets 3, 5, and 2, as this is where our worker nodes that belong to `myipv4cluster` are placed.

!!! note "Important Note"
    These subnets also cover the three AZs for high-availability reasons.

#### Step 3: Identify Security Groups

We can now identify what security groups are being used by these instances using the next command. In our case, the instances are all part of the same security group, as they belong to the same managed node group. We will use this for the EFS mount targets for simplicity, but you may want to create a separate security group for EFS. However, ensure that any security group you use allows the TCP/2049 port between the Pods and the EFS mount targets:

```bash
aws ec2 describe-instances \
  --filters "Name=tag:aws:eks:cluster-name,Values=myipv4cluster" \
  --query "Reservations[*].Instances[*].SecurityGroups[*]" \
  --output table
```

Expected output:
```
+-------------------------+----------------------------------+
|        GroupId          |            GroupName             |
+-------------------------+----------------------------------+
|  sg-123                 |  eks-cluster-sg-myipv4cluster... |
|  sg-123                 |  eks-cluster-sg-myipv4cluster... |
|  sg-123                 |  eks-cluster-sg-myipv4cluster... |
+-------------------------+----------------------------------+
```

#### Step 4: Create Mount Targets

We can now create and verify the mount points, one per subnet/AZ, using the following commands. When we verify the mount targets, you will see the IP address assigned to the Elastic Network Interface (ENI) placed in the subnet, which will be used by the Pods:

```bash
aws efs create-mount-target \
  --file-system-id fs-078166286587fc22 \
  --security-groups sg-123 \
  --subnet-id subnet-3
# Repeat for remaining subnets
```

Verify mount targets:
```bash
aws efs describe-mount-targets \
  --file-system-id fs-078166286587fc22 \
  --query "MountTargets[*].{id:MountTargetId,az:AvailabilityZoneName,subnet:SubnetId,EFSIP:IpAddress}" \
  --output table
```

Expected output:
```
+----------------+----------------+-----------+-----------+
| EFSIP          |      az        |     id    | subnet    |
+----------------+----------------+-----------+-----------+
|  192.168.10.59 |  eu-central-1a |  fsmt-22  |  subnet-3 |
|  192.168.66.201|  eu-central-1c |  fsmt-33  |  subnet-4 |
|  192.168.34.140|  eu-central-1b |  fsmt-44  |  subnet-5 |
+----------------+----------------+-----------+-----------+
```

We have now set up EFS and made it available to the Pods; the next steps are almost identical to EBS and involve setting up the Kubernetes object to use EFS.

### Creating Your EFS Cluster Objects

#### Step 1: Create StorageClass

We need to create the SC and example manifest as shown in the following code snippet. You will need to replace the `fileSystemId` key and then deploy it:

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-078166286587fc22
  directoryPerms: "700"
```

Deploy using:
```bash
kubectl create -f SC-config.yaml
```

#### Step 2: Create PersistentVolumeClaim

We can now create the PVC that consumes the SC using the following manifest. Please note that `accessMode` is now set to `ReadWriteMany`, as this is shared storage:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-claim
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 5Gi
```

Deploy using:
```bash
kubectl create -f pvc.yaml
```

#### Step 3: Verify PVC and PV Creation

If we review the PVC and PV that get created using the commands shown next, we can see the new PVC and the PV are created and bound, as again, we are using dynamic provisioning. This is different from EBS where it's only when the PVC is used that the PV gets created. With EFS, you are charged only for what you use, unlike an EBS volume, which you get charged for as soon as it is created, so there are no issues with creating the PVC/PV combination as soon as the PVC is created:

```bash
kubectl get pvc
kubectl get pv
```

Expected output:
```
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ebs-claim   Bound    pvc-1                                      4Gi        RWO            gp3            14h
efs-claim   Bound    pvc-2                                      5Gi        RWX            efs-sc         4s

NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   AGE
pvc-1   4Gi        RWO            Delete           Bound    default/ebs-claim   gp3            16h
pvc-2   5Gi        RWX            Delete           Bound    default/efs-claim   efs-sc         2m1s
```

If you look in the controller logs (an example is shown next), you can see the CSI driver making a call to create the volume:

```bash
kubectl logs efs-csi-controller-xx -n kube-system -c csi-provisioner --tail 10
```

Expected log output:
```
I1005 10:55:45.301869   1 event.go:282] Event (v1.ObjectReference {Kind:"PersistentVolumeClaim", Namespace:"default", Name:"efs-claim", UID:"323", APIVersion:"v1", ResourceVersion:"15905560", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/efs-claim"
I1005 10:55:46.515609   1 controller.go:838] successfully created PV pvc-2 for PVC efs-claim and csi volume name fs-078166286587fc22::fsap-013a6156108263624
```

#### Step 4: Deploy Pod with EFS Volume

The final step is to provision the Pod to use the PVC and attach the EFS volume to a mount point within the container. The manifest shown next will create a single CentOS-based container and mount the volume under `/data`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: efs-app
spec:
  containers:
    - name: app
      image: centos
      command: ["/bin/sh"]
      args: ["-c", "while true; do echo $(date -u) >> /data/out; sleep 5; done"]
      volumeMounts:
        - name: persistent-storage
          mountPath: /data
  volumes:
    - name: persistent-storage
      persistentVolumeClaim:
        claimName: efs-claim
```

Deploy using:
```bash
kubectl create -f pod2.yaml
```

#### Step 5: Validate Data Persistence

You can validate whether data is being produced and stored in EFS using the following command. If you delete and recreate the Pod, the previous Pod's data will be persisted:

```bash
kubectl exec -it efs-app -- tail /data/out
```

Expected output:
```
Wed Oct 5 20:56:03 UTC 2022
Wed Oct 5 20:56:08 UTC 2022
```

As `Reclaim Policy` is set to `Delete` (by default), if you delete the PVC, you will remove the PV and corresponding EFS data.

## Summary

In this chapter, we explored how EBS (block) differs from EFS (filesystem) storage. We identified that:

### EBS Characteristics
- Normally used where you need to provide dedicated volumes per Pod
- Fixed in size
- Charged as soon as you provision it

### EFS Characteristics  
- Shared storage and can therefore be mounted across multiple Pods
- Can scale as needed
- You are only charged for what you use

### Setup Complexity
We also discussed how EFS requires more setup than EBS, as the EBS filesystem and mount targets need to be deployed prior to it being used in EKS. EFS can be viewed as more complex to set up as it's a shared storage platform, whereas EBS is just network-attached storage for a single node. EBS is generally cheaper to provision and use but it is mostly only used for volumes attached to a single instance (EC2).

### Installation Methods
We then reviewed how to install the CSI drivers:
- Creating an add-on for the EBS CSI driver
- Using Helm for the EFS CSI driver

Once the drivers were installed, we explored the Kubernetes objects (SC, PVC, and PV) and how we can use dynamic provisioning to create the volumes in EBS and EFS from Kubernetes rather than having an administrator provision the volumes for us.

## Key Differences: EBS vs EFS

| Aspect | EBS | EFS |
|--------|-----|-----|
| **Storage Type** | Block storage | File storage (NFS) |
| **Access Pattern** | Single Pod/Instance | Multiple Pods/Instances |
| **Availability** | Single AZ | Multi-AZ |
| **Billing Model** | Provisioned capacity | Used capacity only |
| **Setup Complexity** | Simple (managed add-on) | Complex (requires infrastructure setup) |
| **Performance** | High throughput, dedicated | Shared performance across clients |
| **Primary Use Case** | Dedicated storage per Pod | Shared storage across Pods |
| **Kubernetes Access Mode** | ReadWriteOnce (RWO) | ReadWriteMany (RWX) |
| **Cost** | Generally cheaper for dedicated use | Cost-effective for shared scenarios |

## Troubleshooting Commands

### General Kubernetes Storage Commands
```bash
# Check storage classes
kubectl get storageclass

# Check PVCs and PVs
kubectl get pvc
kubectl get pv

# Describe specific resources for details
kubectl describe pv <pv-name>
kubectl describe pvc <pvc-name>
kubectl describe storageclass <sc-name>
```

### EBS-Specific Commands
```bash
# Check EBS CSI driver pods
kubectl get pods -n kube-system | grep ebs

# Check EBS CSI controller logs
kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner

# Check EBS CSI node logs
kubectl logs daemonset/ebs-csi-node -n kube-system -c ebs-plugin
```

### EFS-Specific Commands
```bash
# Check EFS CSI driver pods
kubectl get pod -n kube-system -l "app.kubernetes.io/name=aws-efs-csi-driver"

# Check EFS CSI controller logs
kubectl logs deployment/efs-csi-controller -n kube-system -c csi-provisioner

# Check EFS mount targets
aws efs describe-mount-targets --file-system-id <filesystem-id>
```

### AWS CLI Commands for Verification
```bash
# List EBS volumes
aws ec2 describe-volumes --filters "Name=tag:kubernetes.io/cluster/<cluster-name>,Values=owned"

# List EFS filesystems
aws efs describe-file-systems

# Check EFS mount targets
aws efs describe-mount-targets --file-system-id <filesystem-id>
```

## Best Practices

### EBS Best Practices
1. **Use gp3 volumes** for better cost-performance ratio
2. **Enable encryption** at rest for security
3. **Consider volume size carefully** as you're charged for provisioned capacity
4. **Use appropriate access modes** (typically ReadWriteOnce)
5. **Set appropriate reclaim policies** (Delete vs Retain)
6. **Monitor volume utilization** to optimize costs

### EFS Best Practices
1. **Create mount targets in all relevant AZs** for high availability
2. **Use appropriate security groups** allowing NFS traffic (port 2049)
3. **Consider EFS performance modes** (General Purpose vs Max I/O)
4. **Use EFS storage classes** (Standard vs Infrequent Access) based on access patterns
5. **Monitor access patterns** to optimize storage class usage
6. **Set appropriate directory permissions** in StorageClass

### General Storage Best Practices
1. **Use dynamic provisioning** for easier management
2. **Implement proper backup strategies** for critical data
3. **Monitor storage costs** regularly
4. **Use appropriate storage classes** for different workloads
5. **Test disaster recovery procedures** regularly
6. **Consider data locality** when choosing between EBS and EFS

## Common Issues and Solutions

### EBS Issues
1. **Volume stuck in "Attaching" state**
   - Check if volume and node are in same AZ
   - Verify CSI driver is running properly
   - Check IAM permissions

2. **Pod stuck in "ContainerCreating" state**
   - Check if EBS volume creation is in progress
   - Verify storage class configuration
   - Check node capacity and limits

### EFS Issues
1. **Mount failures**
   - Verify mount targets exist in Pod's subnet
   - Check security group allows NFS traffic (port 2049)
   - Ensure EFS filesystem is available

2. **Performance issues**
   - Consider switching to Max I/O performance mode
   - Check network throughput limits
   - Monitor concurrent connections

## Security Considerations

### IAM Permissions
- **EBS CSI Driver**: Requires `AmazonEBSCSIDriverPolicy`
- **EFS CSI Driver**: Requires custom policy for EFS operations
- Use **IAM roles for service accounts** (IRSA) for secure authentication

### Network Security
- **EBS**: Uses AWS backbone network, encrypted in transit by default
- **EFS**: Requires NFS traffic (TCP/2049) allowed in security groups
- Consider **VPC endpoints** for private connectivity

### Data Encryption
- **EBS**: Support encryption at rest using AWS KMS
- **EFS**: Supports encryption at rest and in transit
- Configure encryption in StorageClass for automated encryption

## Performance Considerations

### EBS Performance
- **gp3**: Up to 16,000 IOPS, 1,000 MiB/s throughput
- **io1/io2**: Up to 64,000 IOPS with guaranteed performance
- **Performance scales** with volume size for gp2

### EFS Performance
- **General Purpose**: Up to 7,000 file operations per second
- **Max I/O**: Higher levels of aggregate throughput and operations per second
- **Throughput mode**: Provisioned vs Bursting

## Further Reading

- **EBS volume types**: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html)
- **EFS storage classes**: [https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html](https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html)
- **When to use EFS**: [https://aws.amazon.com/efs/when-to-choose-efs/](https://aws.amazon.com/efs/when-to-choose-efs/)
- **EBS multi-attach versus EFS**: [https://www.youtube.com/watch?v=3ORzqOjtsmE](https://www.youtube.com/watch?v=3ORzqOjtsmE)
- **Troubleshooting EFS**: [https://docs.aws.amazon.com/efs/latest/ug/troubleshooting-efs-general.html](https://docs.aws.amazon.com/efs/latest/ug/troubleshooting-efs-general.html)